#!/usr/bin/env python3
"""
å…¨é¢å¤šç½‘ç«™æŠ“å–å™¨ - æŠ“å–æ‰€æœ‰ç›®æ ‡ç½‘ç«™
åŒ…å«AIæœºä¼šå‘ç°çš„æ‰€æœ‰é‡è¦æ•°æ®æº
"""

import asyncio
import random
import json
import time
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from playwright.async_api import async_playwright, Page, BrowserContext
import re


@dataclass
class ScrapingTarget:
    """æŠ“å–ç›®æ ‡é…ç½®"""
    name: str
    category: str  # 'social', 'tech_news', 'startup', 'newsletter', 'reviews'
    priority: int  # 1=highest, 5=lowest
    urls: List[str]
    selectors: Dict[str, List[str]]
    wait_conditions: List[str]
    scroll_strategy: str = "default"
    anti_detection: Dict[str, Any] = None
    success_rate: float = 0.0  # å†å²æˆåŠŸç‡


class ComprehensiveMultiSiteScraper:
    """å…¨é¢å¤šç½‘ç«™æŠ“å–å™¨ - è¦†ç›–æ‰€æœ‰AIæœºä¼šå‘ç°æ•°æ®æº"""
    
    def __init__(self):
        self.browser = None
        self.context = None
        self.playwright = None
        self.results = []
        self.session_id = self._generate_session_id()
        
        # é«˜çº§ç”¨æˆ·ä»£ç†æ± ï¼ˆæ›´çœŸå®çš„ç”¨æˆ·ä»£ç†ï¼‰
        self.user_agents = [
            # Chrome on Windows
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            # Chrome on macOS
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            # Firefox
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0",
            # Safari
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15",
        ]
        
        # è§†çª—å¤§å°æ± 
        self.viewports = [
            {"width": 1920, "height": 1080},
            {"width": 1366, "height": 768}, 
            {"width": 1440, "height": 900},
            {"width": 1536, "height": 864},
            {"width": 1280, "height": 720}
        ]
        
        # å®Œæ•´çš„ç›®æ ‡ç½‘ç«™é…ç½®
        self.targets = self._initialize_comprehensive_targets()
    
    def _generate_session_id(self) -> str:
        """ç”Ÿæˆä¼šè¯ID"""
        return hashlib.md5(str(time.time()).encode()).hexdigest()[:12]
    
    def _initialize_comprehensive_targets(self) -> Dict[str, ScrapingTarget]:
        """åˆå§‹åŒ–æ‰€æœ‰ç›®æ ‡ç½‘ç«™é…ç½®"""
        return {
            # 1. æŠ€æœ¯æ–°é—»ç±» - æœ€é«˜ä¼˜å…ˆçº§
            'hackernews': ScrapingTarget(
                name="Hacker News",
                category="tech_news",
                priority=1,
                urls=["https://news.ycombinator.com/"],
                selectors={
                    'posts': ['tr.athing'],
                    'title': ['span.titleline > a', '.storylink'],
                    'score': ['.score', 'span[id^="score_"]'],
                    'comments': ['a[href*="item?id="]'],
                    'domain': ['.sitestr']
                },
                wait_conditions=['networkidle', 'tr.athing'],
                scroll_strategy="none",
                success_rate=1.0  # 100% æˆåŠŸç‡
            ),
            
            # 2. åˆ›ä¸šäº§å“ç±» - é«˜ä¼˜å…ˆçº§
            'producthunt': ScrapingTarget(
                name="Product Hunt",
                category="startup",
                priority=2,
                urls=["https://www.producthunt.com/"],
                selectors={
                    'posts': ['[data-test*="product"]', 'article', '.item'],
                    'title': ['h3', 'h2', '[data-test="product-name"]'],
                    'description': ['p', '[data-test="product-description"]'],
                    'votes': ['[data-test="vote-button"]', '.vote-count'],
                    'category': ['.category', '[data-test="category"]']
                },
                wait_conditions=['networkidle', 'h3, h2, article'],
                scroll_strategy="smooth",
                anti_detection={'wait_for_js': 5, 'random_scroll': True},
                success_rate=0.7
            ),
            
            # 3. ç‹¬ç«‹å¼€å‘è€…ç¤¾åŒº - é«˜ä¼˜å…ˆçº§
            'indiehackers': ScrapingTarget(
                name="Indie Hackers",
                category="startup",
                priority=2,
                urls=["https://www.indiehackers.com/"],
                selectors={
                    'posts': ['.feed-item', 'article', '.post-item'],
                    'title': ['h2 a', 'h3 a', '.post-title'],
                    'author': ['.author', '[data-test="username"]'],
                    'engagement': ['.engagement', '.stats']
                },
                wait_conditions=['networkidle', '.feed-item, article'],
                scroll_strategy="smooth",
                success_rate=0.6
            ),
            
            # 4. Reddit å¤šç¤¾åŒº - ä¸­ä¼˜å…ˆçº§
            'reddit_entrepreneur': ScrapingTarget(
                name="Reddit - Entrepreneur",
                category="social",
                priority=3,
                urls=["https://www.reddit.com/r/entrepreneur/"],
                selectors={
                    'posts': ['[data-testid^="post-container"]', 'article', '.Post'],
                    'title': ['h3', '[slot="title"]', '[data-adclicklocation="title"]'],
                    'score': ['[data-testid="vote-arrows"] button span', '.score'],
                    'author': ['a[data-testid="post_author_link"]', '.author'],
                    'comments': ['a[data-click-id="comments"]']
                },
                wait_conditions=['networkidle', '[data-testid="post-container"], article'],
                scroll_strategy="infinite",
                anti_detection={'close_popups': True, 'random_mouse_movement': True},
                success_rate=0.4
            ),
            
            'reddit_startups': ScrapingTarget(
                name="Reddit - Startups",
                category="social", 
                priority=3,
                urls=["https://www.reddit.com/r/startups/"],
                selectors={
                    'posts': ['[data-testid^="post-container"]', 'article', '.Post'],
                    'title': ['h3', '[slot="title"]', '[data-adclicklocation="title"]'],
                    'score': ['[data-testid="vote-arrows"] button span', '.score'],
                    'author': ['a[data-testid="post_author_link"]', '.author'],
                    'comments': ['a[data-click-id="comments"]']
                },
                wait_conditions=['networkidle', '[data-testid="post-container"], article'],
                scroll_strategy="infinite", 
                anti_detection={'close_popups': True, 'random_mouse_movement': True},
                success_rate=0.4
            ),
            
            'reddit_saas': ScrapingTarget(
                name="Reddit - SaaS",
                category="social",
                priority=3,
                urls=["https://www.reddit.com/r/SaaS/"],
                selectors={
                    'posts': ['[data-testid^="post-container"]', 'article', '.Post'],
                    'title': ['h3', '[slot="title"]', '[data-adclicklocation="title"]'],
                    'score': ['[data-testid="vote-arrows"] button span', '.score'],
                    'author': ['a[data-testid="post_author_link"]', '.author'],
                    'comments': ['a[data-click-id="comments"]']
                },
                wait_conditions=['networkidle', '[data-testid="post-container"], article'],
                scroll_strategy="infinite",
                anti_detection={'close_popups': True, 'random_mouse_movement': True},
                success_rate=0.4
            ),
            
            # 5. è½¯ä»¶è¯„æµ‹ç±» - ä¸­ä¼˜å…ˆçº§
            'g2_ai': ScrapingTarget(
                name="G2 - AI Software",
                category="reviews",
                priority=3,
                urls=["https://www.g2.com/categories/artificial-intelligence"],
                selectors={
                    'posts': ['.product-listing', '.product-card'],
                    'title': ['.product-name', 'h3 a'],
                    'rating': ['.rating', '.stars'],
                    'reviews_count': ['.review-count'],
                    'category': ['.category-name'],
                    'description': ['.product-description']
                },
                wait_conditions=['networkidle', '.product-listing'],
                scroll_strategy="smooth",
                anti_detection={'wait_for_js': 3},
                success_rate=0.5
            ),
            
            # 6. æŠ€æœ¯åšå®¢å’Œæ–°é—»æº - ä¸­ä¼˜å…ˆçº§
            'betalist': ScrapingTarget(
                name="BetaList",
                category="startup",
                priority=3,
                urls=["https://betalist.com/"],
                selectors={
                    'posts': ['.startup-card', '.startup-item'],
                    'title': ['.startup-name', 'h3'],
                    'description': ['.startup-description'],
                    'category': ['.startup-category'],
                    'status': ['.startup-status']
                },
                wait_conditions=['networkidle', '.startup-card'],
                scroll_strategy="smooth",
                success_rate=0.6
            ),
            
            # 7. å¼€å‘è€…ç¤¾åŒº - ä½ä¼˜å…ˆçº§
            'devto': ScrapingTarget(
                name="Dev.to",
                category="tech_news",
                priority=4,
                urls=["https://dev.to/"],
                selectors={
                    'posts': ['article', '.crayons-story'],
                    'title': ['h2 a', '.crayons-story__title a'],
                    'author': ['.crayons-story__secondary .crayons-link'],
                    'tags': ['.crayons-tag'],
                    'reactions': ['.crayons-story__reaction']
                },
                wait_conditions=['networkidle', 'article'],
                scroll_strategy="smooth",
                success_rate=0.7
            ),
            
            # 8. åˆ›ä¸šåª’ä½“ - ä½ä¼˜å…ˆçº§
            'techcrunch_startups': ScrapingTarget(
                name="TechCrunch Startups",
                category="tech_news",
                priority=4,
                urls=["https://techcrunch.com/category/startups/"],
                selectors={
                    'posts': ['article', '.post-block'],
                    'title': ['h2 a', '.post-block__title a'],
                    'author': ['.river-byline__authors'],
                    'publish_time': ['.river-byline__time'],
                    'excerpt': ['.post-block__content']
                },
                wait_conditions=['networkidle', 'article'],
                scroll_strategy="smooth",
                anti_detection={'wait_for_js': 3},
                success_rate=0.5
            ),
            
            # 9. AngelList/Wellfound - ä¸­ä¼˜å…ˆçº§
            'angellist': ScrapingTarget(
                name="AngelList/Wellfound",
                category="startup",
                priority=3,
                urls=["https://wellfound.com/startups"],
                selectors={
                    'posts': ['.startup-item', '.company-card'],
                    'title': ['.startup-name', 'h3'],
                    'description': ['.startup-pitch'],
                    'stage': ['.startup-stage'],
                    'location': ['.startup-location'],
                    'funding': ['.funding-info']
                },
                wait_conditions=['networkidle', '.startup-item'],
                scroll_strategy="smooth",
                anti_detection={'wait_for_js': 5},
                success_rate=0.4
            ),
            
            # 10. Trends.vc - ä½ä¼˜å…ˆçº§ 
            'trends_vc': ScrapingTarget(
                name="Trends.vc",
                category="newsletter",
                priority=4,
                urls=["https://trends.vc/"],
                selectors={
                    'posts': ['.trend-item', 'article'],
                    'title': ['.trend-title', 'h2'],
                    'category': ['.trend-category'],
                    'description': ['.trend-description']
                },
                wait_conditions=['networkidle', '.trend-item'],
                scroll_strategy="smooth",
                success_rate=0.3
            )
        }
    
    async def setup_ultimate_browser(self) -> None:
        """è®¾ç½®ç»ˆææµè§ˆå™¨ç¯å¢ƒ"""
        print("ğŸš€ åˆå§‹åŒ–å…¨é¢å¤šç½‘ç«™æŠ“å–ç¯å¢ƒ...")
        
        self.playwright = await async_playwright().start()
        
        # éšæœºé€‰æ‹©ç”¨æˆ·ä»£ç†å’Œè§†çª—
        user_agent = random.choice(self.user_agents)
        viewport = random.choice(self.viewports)
        
        print(f"ğŸ­ ä½¿ç”¨ç”¨æˆ·ä»£ç†: {user_agent[:50]}...")
        print(f"ğŸ“± è§†çª—å¤§å°: {viewport['width']}x{viewport['height']}")
        
        # é«˜çº§æµè§ˆå™¨é…ç½®
        self.browser = await self.playwright.chromium.launch(
            headless=False,  # å¯è§†åŒ–æ¨¡å¼ä¾¿äºè°ƒè¯•
            slow_mo=500,     # é€‚ä¸­çš„å»¶è¿Ÿ
            args=[
                '--start-maximized',
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--disable-background-timer-throttling',
                '--disable-backgrounding-occluded-windows',
                '--disable-renderer-backgrounding',
                '--disable-field-trial-config',
                '--disable-back-forward-cache',
                '--disable-ipc-flooding-protection',
                '--enable-automation=false',
                '--no-first-run',
                '--password-store=basic',
                '--use-mock-keychain',
                f'--user-agent={user_agent}'
            ]
        )
        
        # åˆ›å»ºéšèº«ä¸Šä¸‹æ–‡
        self.context = await self.browser.new_context(
            viewport=viewport,
            user_agent=user_agent,
            locale='en-US',
            timezone_id='America/New_York',
            permissions=['geolocation'],
            color_scheme='light',
            reduced_motion='reduce',
            extra_http_headers={
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Cache-Control': 'max-age=0',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1'
            }
        )
        
        # æ³¨å…¥ä¸–ç•Œçº§åæ£€æµ‹è„šæœ¬
        await self.context.add_init_script(self._get_anti_detection_script())
        
        print("âœ… å…¨é¢å¤šç½‘ç«™æŠ“å–ç¯å¢ƒè®¾ç½®å®Œæˆ")
    
    def _get_anti_detection_script(self) -> str:
        """è·å–åæ£€æµ‹è„šæœ¬"""
        return """
            console.log('ğŸ›¡ï¸ åæ£€æµ‹ç³»ç»Ÿå·²æ¿€æ´»');
            
            // 1. ç§»é™¤webdriveræ ‡è¯†
            Object.defineProperty(navigator, 'webdriver', {
                get: () => false,
            });
            
            // 2. æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ç‰¹å¾
            Object.defineProperty(navigator, 'languages', {
                get: () => ['en-US', 'en', 'zh-CN', 'zh'],
            });
            
            Object.defineProperty(navigator, 'plugins', {
                get: () => [
                    {name: 'Chrome PDF Plugin', description: 'Portable Document Format'},
                    {name: 'Chrome PDF Viewer', description: 'PDF Viewer'},
                    {name: 'Native Client', description: 'Native Client'},
                    {name: 'Chromium PDF Plugin', description: 'Portable Document Format'},
                    {name: 'Microsoft Edge PDF Plugin', description: 'PDF Plugin'}
                ],
            });
            
            // 3. è¦†ç›–Permission API
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
            );
            
            // 4. ç§»é™¤Playwrightç‰¹å¾
            delete window.playwright;
            delete window.__playwright;
            delete window._playwright;
            
            // 5. è¦†ç›–console.debug
            const originalDebug = console.debug;
            console.debug = function(...args) {
                if (args[0] && args[0].includes && args[0].includes('playwright')) {
                    return;
                }
                return originalDebug.apply(console, args);
            };
            
            console.log('âœ… åæ£€æµ‹è„šæœ¬é…ç½®å®Œæˆ');
        """
    
    async def scrape_single_target(self, target_id: str, target: ScrapingTarget) -> List[Dict[str, Any]]:
        """æŠ“å–å•ä¸ªç›®æ ‡ç½‘ç«™"""
        print(f"\nğŸ¯ æŠ“å–ç›®æ ‡: {target.name} (ä¼˜å…ˆçº§: {target.priority})")
        print(f"ğŸŒ URLs: {target.urls}")
        
        all_items = []
        
        for url in target.urls:
            items = await self._scrape_single_url(url, target)
            all_items.extend(items)
            
            # å¦‚æœè·å¾—è¶³å¤Ÿæ•°æ®ï¼Œå¯ä»¥æå‰ç»“æŸ
            if len(all_items) >= 15:
                break
        
        # æ›´æ–°æˆåŠŸç‡
        if all_items:
            target.success_rate = min(target.success_rate + 0.1, 1.0)
        else:
            target.success_rate = max(target.success_rate - 0.1, 0.0)
        
        print(f"âœ… {target.name} å®Œæˆ: {len(all_items)} æ¡æ•°æ® (æˆåŠŸç‡: {target.success_rate:.1%})")
        return all_items
    
    async def _scrape_single_url(self, url: str, target: ScrapingTarget) -> List[Dict[str, Any]]:
        """æŠ“å–å•ä¸ªURL"""
        page = await self.context.new_page()
        items = []
        
        try:
            print(f"  ğŸŒ è®¿é—®: {url}")
            
            # è®¿é—®é¡µé¢
            await page.goto(url, wait_until='domcontentloaded', timeout=30000)
            
            # æ™ºèƒ½ç­‰å¾…å’ŒåŠ è½½
            await self._smart_wait_and_load(page, target)
            
            # å¤„ç†å¼¹çª—ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if target.anti_detection and target.anti_detection.get('close_popups'):
                await self._handle_popups(page)
            
            # æ™ºèƒ½æ»šåŠ¨ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if target.anti_detection and target.anti_detection.get('random_scroll'):
                await self._simulate_human_scroll(page)
            
            # æ‰§è¡Œæ»šåŠ¨ç­–ç•¥
            await self._execute_scroll_strategy(page, target.scroll_strategy)
            
            # æ™ºèƒ½æ•°æ®æå–
            items = await self._smart_extract_data(page, target)
            
        except Exception as e:
            print(f"âŒ æŠ“å– {url} å¤±è´¥: {e}")
            # ä¿å­˜è°ƒè¯•æˆªå›¾
            await page.screenshot(path=f'{target.name.lower().replace(" ", "_")}_error_{self.session_id}.png')
            
        finally:
            await page.close()
        
        return items
    
    async def _smart_wait_and_load(self, page: Page, target: ScrapingTarget) -> None:
        """æ™ºèƒ½ç­‰å¾…å’ŒåŠ è½½ç­–ç•¥"""
        # 1. åŸºç¡€ç­‰å¾…
        try:
            await page.wait_for_load_state('networkidle', timeout=15000)
        except:
            print("  âš ï¸ ç½‘ç»œç©ºé—²ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­...")
        
        # 2. å…ƒç´ ç­‰å¾…
        for condition in target.wait_conditions:
            if condition != 'networkidle':
                try:
                    await page.wait_for_selector(condition, timeout=8000)
                    print(f"  âœ… æ‰¾åˆ°å…ƒç´ : {condition}")
                    break
                except:
                    continue
        
        # 3. JavaScriptæ‰§è¡Œç­‰å¾…
        if target.anti_detection and target.anti_detection.get('wait_for_js'):
            await asyncio.sleep(target.anti_detection['wait_for_js'])
    
    async def _handle_popups(self, page: Page) -> None:
        """å¤„ç†å¼¹çª—"""
        popup_selectors = [
            '[aria-label="Close"]', '[aria-label="close"]', 
            'button:has-text("Close")', 'button:has-text("Ã—")',
            '.close-button', '.modal-close',
            'button:has-text("Accept")', 'button:has-text("Got it")'
        ]
        
        for selector in popup_selectors:
            try:
                element = await page.query_selector(selector)
                if element:
                    await element.click()
                    await asyncio.sleep(0.5)
            except:
                continue
    
    async def _simulate_human_scroll(self, page: Page) -> None:
        """æ¨¡æ‹Ÿäººç±»æ»šåŠ¨è¡Œä¸º"""
        for _ in range(3):
            x = random.randint(100, 800)
            y = random.randint(100, 600)
            await page.mouse.move(x, y)
            await asyncio.sleep(random.uniform(0.5, 1.5))
    
    async def _execute_scroll_strategy(self, page: Page, strategy: str) -> None:
        """æ‰§è¡Œæ»šåŠ¨ç­–ç•¥"""
        if strategy == "infinite":
            for i in range(3):
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(random.uniform(2, 3))
        elif strategy == "smooth":
            viewport_height = await page.evaluate("window.innerHeight")
            total_height = await page.evaluate("document.body.scrollHeight")
            
            for position in range(0, min(total_height, viewport_height * 3), viewport_height // 2):
                await page.evaluate(f"window.scrollTo(0, {position})")
                await asyncio.sleep(random.uniform(1, 2))
    
    async def _smart_extract_data(self, page: Page, target: ScrapingTarget) -> List[Dict[str, Any]]:
        """æ™ºèƒ½æ•°æ®æå–"""
        items = []
        
        # å¯»æ‰¾ä¸»è¦å†…å®¹å®¹å™¨
        posts = []
        for selector_list in target.selectors['posts']:
            posts = await page.query_selector_all(selector_list)
            if posts and len(posts) > 2:
                print(f"  âœ… æ‰¾åˆ° {len(posts)} ä¸ªå†…å®¹é¡¹ (é€‰æ‹©å™¨: {selector_list})")
                break
        
        if not posts:
            print("  âš ï¸ æœªæ‰¾åˆ°å†…å®¹é¡¹")
            return items
        
        # æå–æ¯ä¸ªé¡¹ç›®çš„æ•°æ®
        for i, post in enumerate(posts[:15]):  # é™åˆ¶15ä¸ªé¡¹ç›®
            try:
                item_data = {
                    'id': f"{target.name.lower().replace(' ', '_')}_{i}_{int(time.time())}",
                    'source': target.name.lower().replace(' ', '_'),
                    'platform': target.name,
                    'category': target.category,
                    'priority': target.priority,
                    'scraped_at': datetime.now().isoformat(),
                    'session_id': self.session_id,
                    'success_rate': target.success_rate
                }
                
                # æå–å„ç§å­—æ®µ
                for field, selectors in target.selectors.items():
                    if field != 'posts':
                        value = await self._extract_field(post, selectors)
                        if value:
                            item_data[field] = value.strip()[:500]  # é™åˆ¶é•¿åº¦
                
                # è®¡ç®—ç›¸å…³æ€§è¯„åˆ†
                title = item_data.get('title', '')
                if title and len(title) > 5:
                    item_data['relevance_score'] = self._calculate_relevance_score(title)
                    items.append(item_data)
                    print(f"    ğŸ“ {i+1:2d}. {title[:50]}...")
                
            except Exception as e:
                print(f"  âš ï¸ æå–é¡¹ç›® {i} æ—¶å‡ºé”™: {e}")
                continue
        
        return items
    
    async def _extract_field(self, element, selectors: List[str]) -> Optional[str]:
        """ä»å…ƒç´ ä¸­æå–å­—æ®µ"""
        for selector in selectors:
            try:
                field_element = await element.query_selector(selector)
                if field_element:
                    text = await field_element.text_content()
                    if text and text.strip():
                        return text.strip()
            except:
                continue
        return None
    
    def _calculate_relevance_score(self, title: str) -> float:
        """è®¡ç®—å†…å®¹ç›¸å…³æ€§è¯„åˆ†"""
        opportunity_keywords = [
            'startup', 'entrepreneur', 'business', 'opportunity', 'market',
            'saas', 'AI', 'automation', 'platform', 'solution', 'tool',
            'launch', 'funding', 'growth', 'scale', 'innovation', 'app',
            'software', 'tech', 'product', 'service', 'api', 'mobile'
        ]
        
        title_lower = title.lower()
        score = 0.0
        
        for keyword in opportunity_keywords:
            if keyword.lower() in title_lower:
                score += 1.0
        
        return min(score / len(opportunity_keywords), 1.0)
    
    async def run_comprehensive_scraping(self) -> List[Dict[str, Any]]:
        """è¿è¡Œå…¨é¢æŠ“å–æ‰€æœ‰ç½‘ç«™"""
        print("ğŸš€ å¯åŠ¨å…¨é¢å¤šç½‘ç«™æŠ“å–ç³»ç»Ÿ")
        print("è¦†ç›–æ‰€æœ‰AIæœºä¼šå‘ç°æ•°æ®æº")
        print("=" * 80)
        
        await self.setup_ultimate_browser()
        
        all_results = []
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºç›®æ ‡ç½‘ç«™
        sorted_targets = sorted(
            self.targets.items(), 
            key=lambda x: (x[1].priority, -x[1].success_rate)
        )
        
        print(f"\nğŸ“Š æŠ“å–è®¡åˆ’ ({len(sorted_targets)} ä¸ªç½‘ç«™):")
        for target_id, target in sorted_targets:
            print(f"  {target.priority}. {target.name} ({target.category}) - æˆåŠŸç‡: {target.success_rate:.1%}")
        
        # æ‰§è¡ŒæŠ“å–
        for i, (target_id, target) in enumerate(sorted_targets):
            try:
                print(f"\n{'='*60}")
                print(f"è¿›åº¦: {i+1}/{len(sorted_targets)}")
                
                items = await self.scrape_single_target(target_id, target)
                all_results.extend(items)
                
                # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«æ£€æµ‹
                delay = random.uniform(3, 8)
                print(f"â³ ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
                await asyncio.sleep(delay)
                
            except Exception as e:
                print(f"âŒ æŠ“å– {target.name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        self.results = all_results
        return all_results
    
    def analyze_and_display_results(self) -> None:
        """åˆ†æå’Œå±•ç¤ºç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ‰ å…¨é¢å¤šç½‘ç«™æŠ“å–ç³»ç»Ÿæ‰§è¡Œå®Œæˆ!")
        print("="*80)
        
        if not self.results:
            print("âš ï¸ æœªè·å–åˆ°æ•°æ®")
            return
        
        # ç»Ÿè®¡åˆ†æ
        total_items = len(self.results)
        platforms = {}
        categories = {}
        quality_score = 0
        
        for item in self.results:
            platform = item.get('platform', 'unknown')
            category = item.get('category', 'unknown')
            platforms[platform] = platforms.get(platform, 0) + 1
            categories[category] = categories.get(category, 0) + 1
            
            # æ•°æ®è´¨é‡è¯„åˆ†
            score = 0
            if item.get('title'): score += 2
            if item.get('description'): score += 1
            if item.get('score') or item.get('votes'): score += 1
            if item.get('author'): score += 1
            if item.get('relevance_score', 0) > 0.1: score += 1
            quality_score += score
        
        avg_quality = quality_score / total_items if total_items > 0 else 0
        
        print(f"ğŸ“Š æŠ“å–ç»Ÿè®¡:")
        print(f"   æ€»æ•°æ®é‡: {total_items} æ¡")
        print(f"   æ•°æ®è´¨é‡è¯„åˆ†: {avg_quality:.1f}/6.0 â­")
        print(f"   ä¼šè¯ID: {self.session_id}")
        
        print(f"\nğŸ“ˆ å¹³å°åˆ†å¸ƒ:")
        for platform, count in sorted(platforms.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_items) * 100
            print(f"   ğŸŒ {platform:20}: {count:3d} æ¡ ({percentage:5.1f}%)")
        
        print(f"\nğŸ“‚ ç±»åˆ«åˆ†å¸ƒ:")
        for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_items) * 100
            print(f"   ğŸ“ {category:15}: {count:3d} æ¡ ({percentage:5.1f}%)")
        
        print(f"\nğŸ“ é«˜è´¨é‡æ•°æ®æ ·æœ¬:")
        # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åºæ˜¾ç¤º
        sorted_items = sorted(self.results, 
                            key=lambda x: (x.get('relevance_score', 0), len(str(x.get('title', '')))), 
                            reverse=True)
        
        for i, item in enumerate(sorted_items[:15]):
            platform = item.get('platform', '').upper()[:12]
            title = item.get('title', 'No title')[:50]
            relevance = item.get('relevance_score', 0)
            category = item.get('category', '')[:10]
            
            quality_indicators = []
            if item.get('description'): quality_indicators.append('ğŸ“')
            if item.get('author'): quality_indicators.append('ğŸ‘¤')
            if item.get('score') or item.get('votes'): quality_indicators.append('â­')
            if relevance > 0.2: quality_indicators.append(f'ğŸ¯{relevance:.1f}')
            
            indicators = ' '.join(quality_indicators)
            print(f"  {i+1:2d}. [{platform:12}] [{category:10}] {title}... {indicators}")
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"comprehensive_scraping_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                'session_id': self.session_id,
                'timestamp': timestamp,
                'total_items': total_items,
                'quality_score': avg_quality,
                'platforms': platforms,
                'categories': categories,
                'targets_performance': {
                    target_id: {
                        'name': target.name,
                        'success_rate': target.success_rate,
                        'priority': target.priority,
                        'category': target.category
                    } for target_id, target in self.targets.items()
                },
                'data': self.results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        print(f"â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æ˜¾ç¤ºæˆåŠŸç‡ç»Ÿè®¡
        print(f"\nğŸ“ˆ å„ç½‘ç«™æˆåŠŸç‡ç»Ÿè®¡:")
        for target_id, target in sorted(self.targets.items(), key=lambda x: x[1].success_rate, reverse=True):
            status = "ğŸŸ¢" if target.success_rate > 0.7 else "ğŸŸ¡" if target.success_rate > 0.3 else "ğŸ”´"
            print(f"   {status} {target.name:20}: {target.success_rate:6.1%}")
    
    async def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        print("\nğŸ§¹ æ¸…ç†å…¨é¢æŠ“å–ç³»ç»Ÿèµ„æº...")
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        print("âœ… æ¸…ç†å®Œæˆ")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ å…¨é¢å¤šç½‘ç«™AIæœºä¼šå‘ç°æŠ“å–ç³»ç»Ÿ")
    print("ğŸ•·ï¸ è¦†ç›–æ‰€æœ‰é‡è¦æ•°æ®æº")
    print(f"â° å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    scraper = ComprehensiveMultiSiteScraper()
    
    try:
        await scraper.run_comprehensive_scraping()
        scraper.analyze_and_display_results()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await scraper.cleanup()


if __name__ == "__main__":
    asyncio.run(main())