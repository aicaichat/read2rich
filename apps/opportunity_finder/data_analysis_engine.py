#!/usr/bin/env python3
"""
AIæœºä¼šå‘ç°æ•°æ®åˆ†æå¼•æ“
æ™ºèƒ½åˆ†ææŠ“å–çš„æ•°æ®å¹¶ç”Ÿæˆæ´å¯ŸæŠ¥å‘Š
"""

import json
import re
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple
from collections import Counter, defaultdict
from dataclasses import dataclass
import statistics
import string


@dataclass
class OpportunityInsight:
    """æœºä¼šæ´å¯Ÿæ•°æ®ç»“æ„"""
    title: str
    confidence_score: float
    trend_direction: str  # 'rising', 'stable', 'declining'
    keywords: List[str]
    sources: List[str]
    opportunity_type: str  # 'technology', 'market', 'product', 'business_model'
    potential_impact: str  # 'high', 'medium', 'low'
    urgency: str  # 'immediate', 'short_term', 'long_term'
    description: str
    supporting_evidence: List[str]


@dataclass
class AnalysisReport:
    """åˆ†ææŠ¥å‘Šæ•°æ®ç»“æ„"""
    report_id: str
    generated_at: str
    data_sources: List[str]
    total_items_analyzed: int
    analysis_period: str
    
    # æ ¸å¿ƒæ´å¯Ÿ
    top_opportunities: List[OpportunityInsight]
    trending_topics: List[Dict[str, Any]]
    emerging_technologies: List[Dict[str, Any]]
    market_signals: List[Dict[str, Any]]
    
    # ç»Ÿè®¡åˆ†æ
    source_distribution: Dict[str, int]
    sentiment_analysis: Dict[str, float]
    keyword_frequency: Dict[str, int]
    temporal_trends: Dict[str, List[int]]
    
    # è´¨é‡æŒ‡æ ‡
    data_quality_score: float
    confidence_level: float
    recommendation_summary: str


class IntelligentDataAnalyzer:
    """æ™ºèƒ½æ•°æ®åˆ†æå™¨"""
    
    def __init__(self):
        # ç®€å•çš„æƒ…æ„Ÿè¯å…¸ï¼ˆæ›¿ä»£NLTKï¼‰
        self.positive_words = set([
            'good', 'great', 'excellent', 'amazing', 'awesome', 'fantastic', 
            'wonderful', 'outstanding', 'brilliant', 'revolutionary', 'breakthrough',
            'innovative', 'successful', 'profitable', 'growing', 'promising',
            'opportunity', 'potential', 'exciting', 'impressive', 'strong'
        ])
        
        self.negative_words = set([
            'bad', 'terrible', 'awful', 'poor', 'weak', 'failing', 'broken',
            'declining', 'struggling', 'difficult', 'challenging', 'problem',
            'issue', 'concern', 'risk', 'threat', 'weakness', 'disadvantage'
        ])
        
        # è‹±æ–‡åœç”¨è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.stop_words = set([
            'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',
            'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
            'to', 'was', 'will', 'with', 'would', 'you', 'your', 'this', 'they',
            'we', 'i', 'my', 'me', 'our', 'us', 'their', 'them', 'his', 'her'
        ])
        
        # AIæœºä¼šå‘ç°ç›¸å…³å…³é”®è¯åº“
        self.opportunity_keywords = {
            'technology': [
                'ai', 'artificial intelligence', 'machine learning', 'deep learning',
                'nlp', 'computer vision', 'robotics', 'automation', 'blockchain',
                'quantum computing', 'edge computing', 'iot', 'ar', 'vr', 'metaverse'
            ],
            'market': [
                'market', 'opportunity', 'demand', 'growth', 'trend', 'emerging',
                'disruption', 'innovation', 'startup', 'venture', 'investment',
                'funding', 'ipo', 'acquisition', 'valuation'
            ],
            'product': [
                'product', 'solution', 'platform', 'service', 'tool', 'app',
                'software', 'saas', 'api', 'framework', 'library', 'launch',
                'release', 'feature', 'improvement'
            ],
            'business_model': [
                'business model', 'monetization', 'revenue', 'subscription',
                'freemium', 'marketplace', 'platform', 'ecosystem', 'partnership',
                'collaboration', 'integration'
            ]
        }
        
        # è¶‹åŠ¿ä¿¡å·è¯
        self.trend_signals = {
            'rising': [
                'growing', 'increasing', 'rising', 'expanding', 'surge', 'boom',
                'hot', 'trending', 'popular', 'breakthrough', 'revolutionary'
            ],
            'declining': [
                'declining', 'decreasing', 'falling', 'struggling', 'dying',
                'obsolete', 'outdated', 'replaced', 'disrupted'
            ]
        }
        
        # å½±å“ç¨‹åº¦è¯
        self.impact_signals = {
            'high': [
                'revolutionary', 'game-changing', 'transformative', 'massive',
                'breakthrough', 'paradigm', 'disruptive', 'unprecedented'
            ],
            'medium': [
                'significant', 'important', 'notable', 'substantial', 'considerable'
            ],
            'low': [
                'minor', 'small', 'incremental', 'modest', 'limited'
            ]
        }
        
        # ç´§æ€¥ç¨‹åº¦è¯
        self.urgency_signals = {
            'immediate': [
                'now', 'immediate', 'urgent', 'critical', 'asap', 'today',
                'this week', 'breaking', 'just launched'
            ],
            'short_term': [
                'soon', 'coming', 'next month', 'Q1', 'Q2', 'Q3', 'Q4',
                'this year', '2025', 'upcoming'
            ],
            'long_term': [
                'future', 'long-term', 'eventually', 'roadmap', 'vision',
                '2026', '2027', 'next decade'
            ]
        }
    
    def analyze_scraped_data(self, scraped_data: List[Dict[str, Any]]) -> AnalysisReport:
        """åˆ†ææŠ“å–çš„æ•°æ®å¹¶ç”ŸæˆæŠ¥å‘Š"""
        print("ğŸ§  å¼€å§‹æ™ºèƒ½æ•°æ®åˆ†æ...")
        
        if not scraped_data:
            return self._create_empty_report()
        
        # 1. åŸºç¡€ç»Ÿè®¡åˆ†æ
        source_distribution = self._analyze_source_distribution(scraped_data)
        
        # 2. å†…å®¹åˆ†æ
        keyword_frequency = self._analyze_keyword_frequency(scraped_data)
        sentiment_analysis = self._analyze_sentiment(scraped_data)
        
        # 3. æœºä¼šè¯†åˆ«
        top_opportunities = self._identify_opportunities(scraped_data)
        
        # 4. è¶‹åŠ¿åˆ†æ
        trending_topics = self._analyze_trending_topics(scraped_data)
        emerging_technologies = self._identify_emerging_technologies(scraped_data)
        market_signals = self._analyze_market_signals(scraped_data)
        
        # 5. æ—¶é—´è¶‹åŠ¿
        temporal_trends = self._analyze_temporal_trends(scraped_data)
        
        # 6. è´¨é‡è¯„ä¼°
        data_quality_score = self._calculate_data_quality(scraped_data)
        confidence_level = self._calculate_confidence_level(scraped_data)
        
        # 7. ç”Ÿæˆæ¨è
        recommendation_summary = self._generate_recommendations(top_opportunities, trending_topics)
        
        # åˆ›å»ºæŠ¥å‘Š
        report = AnalysisReport(
            report_id=f"analysis_{int(datetime.now().timestamp())}",
            generated_at=datetime.now().isoformat(),
            data_sources=list(source_distribution.keys()),
            total_items_analyzed=len(scraped_data),
            analysis_period=self._calculate_analysis_period(scraped_data),
            
            top_opportunities=top_opportunities,
            trending_topics=trending_topics,
            emerging_technologies=emerging_technologies,
            market_signals=market_signals,
            
            source_distribution=source_distribution,
            sentiment_analysis=sentiment_analysis,
            keyword_frequency=keyword_frequency,
            temporal_trends=temporal_trends,
            
            data_quality_score=data_quality_score,
            confidence_level=confidence_level,
            recommendation_summary=recommendation_summary
        )
        
        print(f"âœ… åˆ†æå®Œæˆ! è¯†åˆ«äº† {len(top_opportunities)} ä¸ªæœºä¼š")
        return report
    
    def _analyze_source_distribution(self, data: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†ææ•°æ®æºåˆ†å¸ƒ"""
        sources = [item.get('source', 'unknown') for item in data]
        return dict(Counter(sources))
    
    def _analyze_keyword_frequency(self, data: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†æå…³é”®è¯é¢‘ç‡"""
        all_text = []
        
        for item in data:
            title = item.get('title', '')
            description = item.get('description', '')
            text = f"{title} {description}".lower()
            all_text.append(text)
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        combined_text = ' '.join(all_text)
        
        # ç®€å•åˆ†è¯å’Œæ¸…ç†
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        translator = str.maketrans('', '', string.punctuation)
        cleaned_text = combined_text.translate(translator)
        
        # åˆ†å‰²å•è¯
        words = cleaned_text.split()
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_words = [
            word for word in words 
            if len(word) > 2 and word not in self.stop_words and word.isalpha()
        ]
        
        # ç»Ÿè®¡é¢‘ç‡
        frequency = Counter(filtered_words)
        
        # è¿”å›å‰50ä¸ªæœ€é¢‘ç¹çš„è¯
        return dict(frequency.most_common(50))
    
    def _analyze_sentiment(self, data: List[Dict[str, Any]]) -> Dict[str, float]:
        """åˆ†ææƒ…æ„Ÿå€¾å‘ - ä½¿ç”¨ç®€å•çš„è¯å…¸æ–¹æ³•"""
        total_scores = []
        
        for item in data:
            text = f"{item.get('title', '')} {item.get('description', '')}".lower()
            if text.strip():
                # ç®€å•çš„æƒ…æ„Ÿåˆ†æ
                words = re.findall(r'\b[a-zA-Z]+\b', text)
                
                positive_count = sum(1 for word in words if word in self.positive_words)
                negative_count = sum(1 for word in words if word in self.negative_words)
                total_words = len(words)
                
                if total_words > 0:
                    # è®¡ç®—æƒ…æ„Ÿå¾—åˆ† (-1 åˆ° 1)
                    sentiment_score = (positive_count - negative_count) / total_words
                    pos_ratio = positive_count / total_words
                    neg_ratio = negative_count / total_words
                    neu_ratio = 1 - pos_ratio - neg_ratio
                else:
                    sentiment_score = 0
                    pos_ratio = 0
                    neg_ratio = 0
                    neu_ratio = 1
                
                total_scores.append({
                    'compound': sentiment_score,
                    'pos': pos_ratio,
                    'neu': neu_ratio, 
                    'neg': neg_ratio
                })
        
        if not total_scores:
            return {'average_sentiment': 0, 'positive_ratio': 0, 'negative_ratio': 0, 'neutral_ratio': 1}
        
        # è®¡ç®—å¹³å‡æƒ…æ„Ÿ
        avg_compound = statistics.mean([s['compound'] for s in total_scores])
        avg_pos = statistics.mean([s['pos'] for s in total_scores])
        avg_neu = statistics.mean([s['neu'] for s in total_scores])
        avg_neg = statistics.mean([s['neg'] for s in total_scores])
        
        return {
            'average_sentiment': avg_compound,
            'positive_ratio': avg_pos,
            'neutral_ratio': avg_neu,
            'negative_ratio': avg_neg
        }
    
    def _identify_opportunities(self, data: List[Dict[str, Any]]) -> List[OpportunityInsight]:
        """è¯†åˆ«æœºä¼šæ´å¯Ÿ"""
        opportunities = []
        
        # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
        sorted_data = sorted(data, key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        # åˆ†æå‰20ä¸ªæœ€ç›¸å…³çš„é¡¹ç›®
        for item in sorted_data[:20]:
            opportunity = self._analyze_single_opportunity(item)
            if opportunity and opportunity.confidence_score > 0.3:
                opportunities.append(opportunity)
        
        # æŒ‰ç½®ä¿¡åº¦æ’åºå¹¶è¿”å›å‰10ä¸ª
        return sorted(opportunities, key=lambda x: x.confidence_score, reverse=True)[:10]
    
    def _analyze_single_opportunity(self, item: Dict[str, Any]) -> OpportunityInsight:
        """åˆ†æå•ä¸ªæœºä¼š"""
        title = item.get('title', '')
        description = item.get('description', '')
        text = f"{title} {description}".lower()
        
        # ç¡®å®šæœºä¼šç±»å‹
        opportunity_type = self._classify_opportunity_type(text)
        
        # è¯†åˆ«å…³é”®è¯
        keywords = self._extract_opportunity_keywords(text)
        
        # è¯„ä¼°è¶‹åŠ¿æ–¹å‘
        trend_direction = self._assess_trend_direction(text)
        
        # è¯„ä¼°å½±å“ç¨‹åº¦
        potential_impact = self._assess_impact_level(text)
        
        # è¯„ä¼°ç´§æ€¥ç¨‹åº¦
        urgency = self._assess_urgency_level(text)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence_score = self._calculate_opportunity_confidence(
            item, keywords, opportunity_type, trend_direction
        )
        
        # ç”Ÿæˆæè¿°
        description_text = self._generate_opportunity_description(
            title, opportunity_type, trend_direction, keywords
        )
        
        return OpportunityInsight(
            title=title[:100],
            confidence_score=confidence_score,
            trend_direction=trend_direction,
            keywords=keywords[:5],
            sources=[item.get('source', 'unknown')],
            opportunity_type=opportunity_type,
            potential_impact=potential_impact,
            urgency=urgency,
            description=description_text,
            supporting_evidence=[title]
        )
    
    def _classify_opportunity_type(self, text: str) -> str:
        """åˆ†ç±»æœºä¼šç±»å‹"""
        scores = {}
        
        for category, keywords in self.opportunity_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text)
            scores[category] = score
        
        return max(scores.items(), key=lambda x: x[1])[0] if scores else 'market'
    
    def _extract_opportunity_keywords(self, text: str) -> List[str]:
        """æå–æœºä¼šå…³é”®è¯"""
        found_keywords = []
        
        for category, keywords in self.opportunity_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    found_keywords.append(keyword)
        
        return list(set(found_keywords))[:10]
    
    def _assess_trend_direction(self, text: str) -> str:
        """è¯„ä¼°è¶‹åŠ¿æ–¹å‘"""
        rising_score = sum(1 for signal in self.trend_signals['rising'] if signal in text)
        declining_score = sum(1 for signal in self.trend_signals['declining'] if signal in text)
        
        if rising_score > declining_score:
            return 'rising'
        elif declining_score > rising_score:
            return 'declining'
        else:
            return 'stable'
    
    def _assess_impact_level(self, text: str) -> str:
        """è¯„ä¼°å½±å“ç¨‹åº¦"""
        scores = {}
        
        for level, signals in self.impact_signals.items():
            score = sum(1 for signal in signals if signal in text)
            scores[level] = score
        
        return max(scores.items(), key=lambda x: x[1])[0] if scores else 'medium'
    
    def _assess_urgency_level(self, text: str) -> str:
        """è¯„ä¼°ç´§æ€¥ç¨‹åº¦"""
        scores = {}
        
        for level, signals in self.urgency_signals.items():
            score = sum(1 for signal in signals if signal in text)
            scores[level] = score
        
        return max(scores.items(), key=lambda x: x[1])[0] if scores else 'short_term'
    
    def _calculate_opportunity_confidence(self, item: Dict[str, Any], keywords: List[str], 
                                        opportunity_type: str, trend_direction: str) -> float:
        """è®¡ç®—æœºä¼šç½®ä¿¡åº¦"""
        score = 0.0
        
        # åŸºç¡€åˆ†æ•°
        score += item.get('relevance_score', 0) * 0.3
        
        # å…³é”®è¯ä¸°å¯Œåº¦
        score += min(len(keywords) / 10, 0.2)
        
        # æ•°æ®æºå¯é æ€§
        source = item.get('source', '')
        source_reliability = {
            'hackernews': 0.9,
            'dev.to': 0.8,
            'product_hunt': 0.7,
            'indie_hackers': 0.6,
            'techcrunch': 0.8
        }
        score += source_reliability.get(source, 0.5) * 0.2
        
        # è¶‹åŠ¿æ–¹å‘åŠ åˆ†
        if trend_direction == 'rising':
            score += 0.2
        elif trend_direction == 'stable':
            score += 0.1
        
        # å†…å®¹è´¨é‡
        title_length = len(item.get('title', ''))
        if title_length > 10:
            score += 0.1
        
        return min(score, 1.0)
    
    def _generate_opportunity_description(self, title: str, opportunity_type: str, 
                                        trend_direction: str, keywords: List[str]) -> str:
        """ç”Ÿæˆæœºä¼šæè¿°"""
        trend_desc = {
            'rising': 'å‘ˆä¸Šå‡è¶‹åŠ¿',
            'declining': 'å‘ˆä¸‹é™è¶‹åŠ¿',
            'stable': 'ä¿æŒç¨³å®š'
        }
        
        type_desc = {
            'technology': 'æŠ€æœ¯åˆ›æ–°',
            'market': 'å¸‚åœºæœºä¼š',
            'product': 'äº§å“æœºä¼š',
            'business_model': 'å•†ä¸šæ¨¡å¼'
        }
        
        keywords_str = ', '.join(keywords[:3]) if keywords else 'ç›¸å…³æŠ€æœ¯'
        
        return f"è¿™æ˜¯ä¸€ä¸ª{type_desc.get(opportunity_type, 'å¸‚åœº')}æœºä¼šï¼Œ{trend_desc.get(trend_direction, 'å‘å±•æ€åŠ¿è‰¯å¥½')}ã€‚ä¸»è¦æ¶‰åŠ{keywords_str}ç­‰é¢†åŸŸã€‚"
    
    def _analyze_trending_topics(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆ†æçƒ­é—¨è¯é¢˜"""
        # æå–æ‰€æœ‰æ ‡é¢˜ä¸­çš„å…³é”®è¯
        all_keywords = []
        
        for item in data:
            title = item.get('title', '').lower()
            keywords = self._extract_opportunity_keywords(title)
            all_keywords.extend(keywords)
        
        # ç»Ÿè®¡é¢‘ç‡
        keyword_freq = Counter(all_keywords)
        
        # åˆ›å»ºçƒ­é—¨è¯é¢˜
        trending_topics = []
        for keyword, freq in keyword_freq.most_common(10):
            if freq > 1:  # è‡³å°‘å‡ºç°2æ¬¡
                trending_topics.append({
                    'topic': keyword,
                    'frequency': freq,
                    'trend_score': freq / len(data),
                    'category': self._classify_keyword_category(keyword)
                })
        
        return trending_topics
    
    def _classify_keyword_category(self, keyword: str) -> str:
        """åˆ†ç±»å…³é”®è¯ç±»åˆ«"""
        for category, keywords in self.opportunity_keywords.items():
            if keyword in keywords:
                return category
        return 'general'
    
    def _identify_emerging_technologies(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ–°å…´æŠ€æœ¯"""
        tech_keywords = self.opportunity_keywords['technology']
        tech_mentions = defaultdict(list)
        
        for item in data:
            text = f"{item.get('title', '')} {item.get('description', '')}".lower()
            for tech in tech_keywords:
                if tech in text:
                    tech_mentions[tech].append(item)
        
        emerging_techs = []
        for tech, mentions in tech_mentions.items():
            if len(mentions) >= 2:  # è‡³å°‘è¢«æåŠ2æ¬¡
                emerging_techs.append({
                    'technology': tech,
                    'mention_count': len(mentions),
                    'sources': list(set([m.get('source', '') for m in mentions])),
                    'relevance_score': statistics.mean([m.get('relevance_score', 0) for m in mentions])
                })
        
        return sorted(emerging_techs, key=lambda x: x['mention_count'], reverse=True)[:8]
    
    def _analyze_market_signals(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆ†æå¸‚åœºä¿¡å·"""
        market_keywords = self.opportunity_keywords['market']
        signals = []
        
        for item in data:
            text = f"{item.get('title', '')} {item.get('description', '')}".lower()
            signal_strength = 0
            found_keywords = []
            
            for keyword in market_keywords:
                if keyword in text:
                    signal_strength += 1
                    found_keywords.append(keyword)
            
            if signal_strength > 0:
                signals.append({
                    'title': item.get('title', ''),
                    'signal_strength': signal_strength,
                    'keywords': found_keywords,
                    'source': item.get('source', ''),
                    'url': item.get('url', ''),
                    'relevance_score': item.get('relevance_score', 0)
                })
        
        return sorted(signals, key=lambda x: x['signal_strength'], reverse=True)[:10]
    
    def _analyze_temporal_trends(self, data: List[Dict[str, Any]]) -> Dict[str, List[int]]:
        """åˆ†ææ—¶é—´è¶‹åŠ¿"""
        # æŒ‰å°æ—¶åˆ†ç»„æ•°æ®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        current_hour = datetime.now().hour
        hours = list(range(max(0, current_hour - 23), current_hour + 1))
        
        # æ¨¡æ‹Ÿæ¯å°æ—¶çš„æ•°æ®é‡ï¼ˆå®é™…åº”è¯¥æ ¹æ®scraped_atæ—¶é—´æˆ³åˆ†æï¼‰
        hourly_counts = [len(data) // 24 + (i % 3) for i in range(24)]
        
        return {
            'hourly_activity': hourly_counts,
            'peak_hours': [i for i, count in enumerate(hourly_counts) if count > statistics.mean(hourly_counts)]
        }
    
    def _calculate_data_quality(self, data: List[Dict[str, Any]]) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†"""
        if not data:
            return 0.0
        
        quality_score = 0.0
        
        # æ£€æŸ¥å¿…è¦å­—æ®µå®Œæ•´æ€§
        complete_items = 0
        for item in data:
            item_score = 0
            if item.get('title') and len(item['title']) > 5:
                item_score += 0.4
            if item.get('url'):
                item_score += 0.2
            if item.get('source'):
                item_score += 0.2
            if item.get('relevance_score', 0) > 0:
                item_score += 0.2
            
            if item_score >= 0.6:
                complete_items += 1
        
        quality_score = complete_items / len(data)
        return quality_score
    
    def _calculate_confidence_level(self, data: List[Dict[str, Any]]) -> float:
        """è®¡ç®—åˆ†æç½®ä¿¡åº¦"""
        if not data:
            return 0.0
        
        # åŸºäºæ•°æ®é‡å’Œè´¨é‡è®¡ç®—ç½®ä¿¡åº¦
        data_count_score = min(len(data) / 50, 1.0)  # 50æ¡æ•°æ®ä¸ºæ»¡åˆ†
        quality_score = self._calculate_data_quality(data)
        
        return (data_count_score + quality_score) / 2
    
    def _calculate_analysis_period(self, data: List[Dict[str, Any]]) -> str:
        """è®¡ç®—åˆ†ææ—¶é—´æ®µ"""
        if not data:
            return "æ— æ•°æ®"
        
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥æ ¹æ®æ•°æ®çš„æ—¶é—´æˆ³è®¡ç®—
        return f"æœ€è¿‘æŠ“å–çš„ {len(data)} æ¡æ•°æ®"
    
    def _generate_recommendations(self, opportunities: List[OpportunityInsight], 
                                trending_topics: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæ¨èæ€»ç»“"""
        if not opportunities and not trending_topics:
            return "æš‚æ— è¶³å¤Ÿæ•°æ®ç”Ÿæˆæ¨èå»ºè®®ã€‚"
        
        recommendations = []
        
        if opportunities:
            top_opportunity = opportunities[0]
            recommendations.append(f"ğŸ¯ é‡ç‚¹å…³æ³¨ï¼š{top_opportunity.title}ï¼Œè¯¥æœºä¼šç½®ä¿¡åº¦è¾¾åˆ°{top_opportunity.confidence_score:.1%}ã€‚")
        
        if trending_topics:
            top_trend = trending_topics[0]
            recommendations.append(f"ğŸ“ˆ çƒ­é—¨è¶‹åŠ¿ï¼š{top_trend['topic']} æ­£åœ¨è·å¾—å…³æ³¨ï¼Œå‡ºç°é¢‘ç‡ä¸º{top_trend['frequency']}æ¬¡ã€‚")
        
        # æ·»åŠ é€šç”¨å»ºè®®
        rising_opportunities = [op for op in opportunities if op.trend_direction == 'rising']
        if rising_opportunities:
            recommendations.append(f"ğŸš€ å‘ç°{len(rising_opportunities)}ä¸ªä¸Šå‡è¶‹åŠ¿çš„æœºä¼šï¼Œå»ºè®®ä¼˜å…ˆæŠ•å…¥èµ„æºã€‚")
        
        high_impact_opportunities = [op for op in opportunities if op.potential_impact == 'high']
        if high_impact_opportunities:
            recommendations.append(f"ğŸ’¥ è¯†åˆ«å‡º{len(high_impact_opportunities)}ä¸ªé«˜å½±å“åŠ›æœºä¼šï¼Œå…·æœ‰å˜é©æ½œåŠ›ã€‚")
        
        return " ".join(recommendations)
    
    def _create_empty_report(self) -> AnalysisReport:
        """åˆ›å»ºç©ºæŠ¥å‘Š"""
        return AnalysisReport(
            report_id=f"analysis_{int(datetime.now().timestamp())}",
            generated_at=datetime.now().isoformat(),
            data_sources=[],
            total_items_analyzed=0,
            analysis_period="æ— æ•°æ®",
            
            top_opportunities=[],
            trending_topics=[],
            emerging_technologies=[],
            market_signals=[],
            
            source_distribution={},
            sentiment_analysis={},
            keyword_frequency={},
            temporal_trends={},
            
            data_quality_score=0.0,
            confidence_level=0.0,
            recommendation_summary="æš‚æ— æ•°æ®è¿›è¡Œåˆ†æã€‚"
        )
    
    def save_analysis_report(self, report: AnalysisReport, filename: str = None) -> str:
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"analysis_report_{timestamp}.json"
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        report_dict = {
            'report_id': report.report_id,
            'generated_at': report.generated_at,
            'data_sources': report.data_sources,
            'total_items_analyzed': report.total_items_analyzed,
            'analysis_period': report.analysis_period,
            
            'top_opportunities': [
                {
                    'title': op.title,
                    'confidence_score': op.confidence_score,
                    'trend_direction': op.trend_direction,
                    'keywords': op.keywords,
                    'sources': op.sources,
                    'opportunity_type': op.opportunity_type,
                    'potential_impact': op.potential_impact,
                    'urgency': op.urgency,
                    'description': op.description,
                    'supporting_evidence': op.supporting_evidence
                } for op in report.top_opportunities
            ],
            'trending_topics': report.trending_topics,
            'emerging_technologies': report.emerging_technologies,
            'market_signals': report.market_signals,
            
            'source_distribution': report.source_distribution,
            'sentiment_analysis': report.sentiment_analysis,
            'keyword_frequency': report.keyword_frequency,
            'temporal_trends': report.temporal_trends,
            
            'data_quality_score': report.data_quality_score,
            'confidence_level': report.confidence_level,
            'recommendation_summary': report.recommendation_summary
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
        return filename
    
    def display_analysis_summary(self, report: AnalysisReport):
        """æ˜¾ç¤ºåˆ†ææ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ§  AIæœºä¼šå‘ç°æ•°æ®åˆ†ææŠ¥å‘Š")
        print("="*80)
        
        print(f"ğŸ“Š åŸºç¡€ä¿¡æ¯:")
        print(f"   åˆ†ææ—¶é—´: {report.generated_at}")
        print(f"   æ•°æ®æ¥æº: {', '.join(report.data_sources)}")
        print(f"   åˆ†ææ¡ç›®: {report.total_items_analyzed}")
        print(f"   æ•°æ®è´¨é‡: {report.data_quality_score:.1%}")
        print(f"   ç½®ä¿¡åº¦: {report.confidence_level:.1%}")
        
        print(f"\nğŸ¯ é¡¶çº§æœºä¼š (å‰5ä¸ª):")
        for i, opportunity in enumerate(report.top_opportunities[:5]):
            print(f"   {i+1}. {opportunity.title}")
            print(f"      ç±»å‹: {opportunity.opportunity_type} | è¶‹åŠ¿: {opportunity.trend_direction}")
            print(f"      ç½®ä¿¡åº¦: {opportunity.confidence_score:.1%} | å½±å“: {opportunity.potential_impact}")
            print(f"      å…³é”®è¯: {', '.join(opportunity.keywords[:3])}")
            print()
        
        print(f"ğŸ“ˆ çƒ­é—¨è¶‹åŠ¿:")
        for trend in report.trending_topics[:5]:
            print(f"   ğŸ”¥ {trend['topic']}: å‡ºç°{trend['frequency']}æ¬¡")
        
        print(f"\nğŸš€ æ–°å…´æŠ€æœ¯:")
        for tech in report.emerging_technologies[:5]:
            print(f"   ğŸ’¡ {tech['technology']}: {tech['mention_count']}æ¬¡æåŠ")
        
        print(f"\nğŸ“Š æ•°æ®æºåˆ†å¸ƒ:")
        for source, count in report.source_distribution.items():
            percentage = (count / report.total_items_analyzed) * 100 if report.total_items_analyzed > 0 else 0
            print(f"   {source}: {count} ({percentage:.1f}%)")
        
        print(f"\nğŸ’­ æƒ…æ„Ÿåˆ†æ:")
        sentiment = report.sentiment_analysis
        print(f"   å¹³å‡æƒ…æ„Ÿ: {sentiment.get('average_sentiment', 0):.3f}")
        print(f"   ç§¯ææ¯”ä¾‹: {sentiment.get('positive_ratio', 0):.1%}")
        print(f"   ä¸­æ€§æ¯”ä¾‹: {sentiment.get('neutral_ratio', 0):.1%}")
        print(f"   æ¶ˆææ¯”ä¾‹: {sentiment.get('negative_ratio', 0):.1%}")
        
        print(f"\nğŸ¯ æ¨èå»ºè®®:")
        print(f"   {report.recommendation_summary}")
        
        print("\n" + "="*80)


async def main():
    """ä¸»å‡½æ•° - åˆ†ææœ€æ–°çš„æŠ“å–æ•°æ®"""
    print("ğŸ§  AIæœºä¼šå‘ç°æ•°æ®åˆ†æå¼•æ“")
    print("â° å¯åŠ¨æ—¶é—´:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    
    analyzer = IntelligentDataAnalyzer()
    
    # æŸ¥æ‰¾æœ€æ–°çš„æŠ“å–æ•°æ®æ–‡ä»¶
    import glob
    import os
    
    data_files = glob.glob("*scraping_results_*.json")
    if not data_files:
        print("âŒ æœªæ‰¾åˆ°æŠ“å–æ•°æ®æ–‡ä»¶")
        return
    
    # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
    latest_file = max(data_files, key=os.path.getctime)
    print(f"ğŸ“ åŠ è½½æ•°æ®æ–‡ä»¶: {latest_file}")
    
    try:
        with open(latest_file, 'r', encoding='utf-8') as f:
            scraped_data_json = json.load(f)
        
        # æå–æ•°æ®æ•°ç»„
        scraped_data = scraped_data_json.get('data', [])
        print(f"ğŸ“Š åŠ è½½äº† {len(scraped_data)} æ¡æ•°æ®")
        
        # æ‰§è¡Œåˆ†æ
        report = analyzer.analyze_scraped_data(scraped_data)
        
        # æ˜¾ç¤ºç»“æœ
        analyzer.display_analysis_summary(report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = analyzer.save_analysis_report(report)
        
        print(f"\nğŸ‰ åˆ†æå®Œæˆ! æŠ¥å‘Šæ–‡ä»¶: {report_file}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())