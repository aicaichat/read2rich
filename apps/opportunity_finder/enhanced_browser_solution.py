#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ“å–è§£å†³æ–¹æ¡ˆ
è¿™æ˜¯æœ€ä¼˜çš„ç°ä»£ç½‘ç«™æŠ“å–æ–¹æ³•
"""

import asyncio
import random
from datetime import datetime
from playwright.async_api import async_playwright
import json


class EnhancedBrowserScraper:
    """å¢å¼ºç‰ˆæµè§ˆå™¨æŠ“å–å™¨ - ä¸“æ³¨äºç°ä»£ç½‘ç«™æŠ“å–"""
    
    def __init__(self):
        self.browser = None
        self.context = None
        self.results = []
    
    async def setup_browser(self):
        """è®¾ç½®é«˜çº§æµè§ˆå™¨ç¯å¢ƒ"""
        print("ğŸ”§ åˆå§‹åŒ–å¢å¼ºç‰ˆæµè§ˆå™¨ç¯å¢ƒ...")
        
        self.playwright = await async_playwright().start()
        
        # é«˜çº§æµè§ˆå™¨é…ç½®
        self.browser = await self.playwright.chromium.launch(
            headless=False,          # å¯è§†åŒ–è¿è¡Œ
            slow_mo=1500,           # åŠ¨ä½œé—´å»¶è¿Ÿ1.5ç§’
            args=[
                '--start-maximized',
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor'
            ]
        )
        
        # åˆ›å»ºéšèº«ä¸Šä¸‹æ–‡ï¼Œæ¯æ¬¡éƒ½æ˜¯"æ–°ç”¨æˆ·"
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            locale='en-US',
            timezone_id='America/New_York'
        )
        
        # æ³¨å…¥åæ£€æµ‹è„šæœ¬
        await self.context.add_init_script("""
            // éšè—webdriverç‰¹å¾
            Object.defineProperty(navigator, 'webdriver', {
                get: () => false,
            });
            
            // æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ç‰¹å¾
            Object.defineProperty(navigator, 'languages', {
                get: () => ['en-US', 'en'],
            });
            
            // æ¨¡æ‹Ÿæ’ä»¶
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5],
            });
            
            // è¦†ç›–Permission API
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
            );
            
            console.log('ğŸ•µï¸ åæ£€æµ‹è„šæœ¬å·²åŠ è½½');
        """)
        
        print("âœ… æµè§ˆå™¨ç¯å¢ƒè®¾ç½®å®Œæˆ")
    
    async def scrape_reddit_enhanced(self, subreddit='entrepreneur'):
        """å¢å¼ºç‰ˆRedditæŠ“å–"""
        print(f"\nğŸ“± å¼€å§‹å¢å¼ºæŠ“å– Reddit r/{subreddit}...")
        
        page = await self.context.new_page()
        items = []
        
        try:
            # è®¿é—®Reddit
            url = f"https://www.reddit.com/r/{subreddit}/"
            print(f"ğŸŒ è®¿é—®: {url}")
            await page.goto(url, wait_until='networkidle', timeout=30000)
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            await asyncio.sleep(3)
            
            # å°è¯•å…³é—­å¼¹çª—
            try:
                close_buttons = ['[aria-label="Close"]', 'button:has-text("Close")', '[data-testid="close-button"]']
                for selector in close_buttons:
                    if await page.query_selector(selector):
                        await page.click(selector)
                        await asyncio.sleep(1)
                        break
            except:
                pass
            
            # æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
            print("ğŸ“œ æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹...")
            for i in range(3):
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(2)
            
            # å¯»æ‰¾å¸–å­ - å°è¯•å¤šç§é€‰æ‹©å™¨
            post_selectors = [
                '[data-testid="post-container"]',
                'article',
                'div[data-click-id="body"]',
                'h3',
                '.Post'
            ]
            
            posts = []
            for selector in post_selectors:
                posts = await page.query_selector_all(selector)
                if posts:
                    print(f"âœ… æ‰¾åˆ° {len(posts)} ä¸ªå¸–å­ (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                    break
            
            if not posts:
                print("âš ï¸  æœªæ‰¾åˆ°å¸–å­ï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–ç½‘ç«™ç»“æ„å·²å˜åŒ–")
                # æˆªå›¾ç”¨äºè°ƒè¯•
                await page.screenshot(path=f'reddit_debug_{subreddit}.png')
                print("ğŸ“¸ å·²ä¿å­˜è°ƒè¯•æˆªå›¾")
            
            # æå–å¸–å­ä¿¡æ¯
            for i, post in enumerate(posts[:10]):
                try:
                    # æå–æ ‡é¢˜
                    title_selectors = ['h3', '[slot="title"]', 'a[data-click-id="body"]']
                    title = ""
                    for sel in title_selectors:
                        title_elem = await post.query_selector(sel)
                        if title_elem:
                            title = await title_elem.text_content()
                            if title and len(title.strip()) > 5:
                                break
                    
                    if title and len(title.strip()) > 5:
                        item = {
                            'id': f"reddit_{subreddit}_{i}_{int(datetime.now().timestamp())}",
                            'title': title.strip()[:200],
                            'source': f'reddit_r_{subreddit}',
                            'platform': 'reddit',
                            'scraped_at': datetime.now().isoformat(),
                            'method': 'enhanced_browser'
                        }
                        items.append(item)
                        print(f"  ğŸ“ {i+1:2d}. {title.strip()[:60]}...")
                
                except Exception as e:
                    print(f"âš ï¸  æå–å¸–å­{i}æ—¶å‡ºé”™: {e}")
                    continue
            
            print(f"âœ… Reddit r/{subreddit} æŠ“å–å®Œæˆ: {len(items)} æ¡æ•°æ®")
            
        except Exception as e:
            print(f"âŒ RedditæŠ“å–å¤±è´¥: {e}")
            await page.screenshot(path=f'reddit_error_{subreddit}.png')
        
        finally:
            await page.close()
        
        return items
    
    async def scrape_hackernews_enhanced(self):
        """å¢å¼ºç‰ˆHackerNewsæŠ“å–"""
        print(f"\nğŸ“° å¼€å§‹å¢å¼ºæŠ“å– Hacker News...")
        
        page = await self.context.new_page()
        items = []
        
        try:
            print("ğŸŒ è®¿é—®: https://news.ycombinator.com/")
            await page.goto("https://news.ycombinator.com/", wait_until='networkidle')
            await asyncio.sleep(2)
            
            # HackerNewsç›¸å¯¹ç®€å•ï¼Œç›´æ¥æŠ“å–
            stories = await page.query_selector_all('span.titleline > a')
            print(f"âœ… æ‰¾åˆ° {len(stories)} ä¸ªæ•…äº‹")
            
            for i, story in enumerate(stories[:15]):
                try:
                    title = await story.text_content()
                    href = await story.get_attribute('href')
                    
                    if title:
                        # è·å–è¯„åˆ†ä¿¡æ¯
                        score = 0
                        try:
                            score_elem = await page.query_selector(f'#score_{i}')
                            if score_elem:
                                score_text = await score_elem.text_content()
                                score = int(''.join(filter(str.isdigit, score_text))) if score_text else 0
                        except:
                            pass
                        
                        item = {
                            'id': f"hn_{i}_{int(datetime.now().timestamp())}",
                            'title': title.strip(),
                            'url': href if href and href.startswith('http') else f"https://news.ycombinator.com/{href}",
                            'score': score,
                            'source': 'hackernews',
                            'platform': 'hackernews',
                            'scraped_at': datetime.now().isoformat(),
                            'method': 'enhanced_browser'
                        }
                        items.append(item)
                        print(f"  ğŸ“ {i+1:2d}. {title.strip()[:60]}... (è¯„åˆ†: {score})")
                
                except Exception as e:
                    print(f"âš ï¸  æå–æ•…äº‹{i}æ—¶å‡ºé”™: {e}")
                    continue
            
            print(f"âœ… HackerNewsæŠ“å–å®Œæˆ: {len(items)} æ¡æ•°æ®")
            
        except Exception as e:
            print(f"âŒ HackerNewsæŠ“å–å¤±è´¥: {e}")
            await page.screenshot(path='hn_error.png')
        
        finally:
            await page.close()
        
        return items
    
    async def scrape_producthunt_enhanced(self):
        """å¢å¼ºç‰ˆProduct HuntæŠ“å–"""
        print(f"\nğŸš€ å¼€å§‹å¢å¼ºæŠ“å– Product Hunt...")
        
        page = await self.context.new_page()
        items = []
        
        try:
            print("ğŸŒ è®¿é—®: https://www.producthunt.com/")
            await page.goto("https://www.producthunt.com/", wait_until='networkidle')
            await asyncio.sleep(3)
            
            # ç­‰å¾…äº§å“åŠ è½½
            try:
                await page.wait_for_selector('h3, h2, [data-test]', timeout=10000)
            except:
                print("âš ï¸  é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­...")
            
            # æ»šåŠ¨åŠ è½½
            for i in range(2):
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(2)
            
            # å¯»æ‰¾äº§å“ - å°è¯•å¤šç§é€‰æ‹©å™¨
            product_selectors = ['h3', 'h2', '[data-test*="product"]', 'article']
            products = []
            
            for selector in product_selectors:
                products = await page.query_selector_all(selector)
                if len(products) > 5:  # æ‰¾åˆ°è¶³å¤Ÿçš„äº§å“
                    print(f"âœ… æ‰¾åˆ° {len(products)} ä¸ªäº§å“ (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                    break
            
            if not products:
                print("âš ï¸  æœªæ‰¾åˆ°äº§å“ï¼Œä¿å­˜è°ƒè¯•æˆªå›¾...")
                await page.screenshot(path='ph_debug.png')
            
            # æå–äº§å“ä¿¡æ¯
            for i, product in enumerate(products[:10]):
                try:
                    title = await product.text_content()
                    
                    if title and len(title.strip()) > 3 and len(title.strip()) < 100:
                        item = {
                            'id': f"ph_{i}_{int(datetime.now().timestamp())}",
                            'title': title.strip(),
                            'source': 'product_hunt',
                            'platform': 'product_hunt',
                            'scraped_at': datetime.now().isoformat(),
                            'method': 'enhanced_browser'
                        }
                        items.append(item)
                        print(f"  ğŸ“ {i+1:2d}. {title.strip()[:60]}...")
                
                except Exception as e:
                    continue
            
            print(f"âœ… Product HuntæŠ“å–å®Œæˆ: {len(items)} æ¡æ•°æ®")
            
        except Exception as e:
            print(f"âŒ Product HuntæŠ“å–å¤±è´¥: {e}")
            await page.screenshot(path='ph_error.png')
        
        finally:
            await page.close()
        
        return items
    
    async def run_comprehensive_scraping(self):
        """è¿è¡Œå…¨é¢çš„å¢å¼ºæŠ“å–"""
        print("ğŸ­ å¼€å§‹å…¨é¢å¢å¼ºæµè§ˆå™¨æŠ“å–")
        print("=" * 60)
        
        await self.setup_browser()
        
        all_items = []
        
        # æŠ“å–å¤šä¸ªRedditç¤¾åŒº
        reddit_communities = ['entrepreneur', 'startups', 'SaaS']
        for community in reddit_communities:
            items = await self.scrape_reddit_enhanced(community)
            all_items.extend(items)
            await asyncio.sleep(3)  # ç¤¾åŒºé—´å»¶è¿Ÿ
        
        # æŠ“å–HackerNews
        hn_items = await self.scrape_hackernews_enhanced()
        all_items.extend(hn_items)
        await asyncio.sleep(3)
        
        # æŠ“å–Product Hunt
        ph_items = await self.scrape_producthunt_enhanced()
        all_items.extend(ph_items)
        
        self.results = all_items
        return all_items
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        print("\nğŸ§¹ æ¸…ç†æµè§ˆå™¨èµ„æº...")
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        print("âœ… æ¸…ç†å®Œæˆ")
    
    def display_results(self):
        """å±•ç¤ºç»“æœ"""
        print("\nğŸ‰ å¢å¼ºæµè§ˆå™¨æŠ“å–å®Œæˆ!")
        print("=" * 60)
        print(f"ğŸ“Š æ€»å…±è·å–æ•°æ®: {len(self.results)} æ¡")
        
        if self.results:
            # æŒ‰å¹³å°åˆ†ç»„
            platforms = {}
            for item in self.results:
                platform = item.get('platform', 'unknown')
                platforms[platform] = platforms.get(platform, 0) + 1
            
            print(f"\nğŸ“ˆ æ•°æ®æºåˆ†å¸ƒ:")
            for platform, count in platforms.items():
                print(f"  ğŸŒ {platform:15}: {count:3d} æ¡")
            
            print(f"\nğŸ“ æœ€æ–°æ•°æ®æ ·æœ¬ (å‰10æ¡):")
            for i, item in enumerate(self.results[:10]):
                platform = item.get('platform', '').upper()
                title = item.get('title', 'No title')[:50]
                print(f"  {i+1:2d}. [{platform:12}] {title}...")
            
            # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"enhanced_scraping_results_{timestamp}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        
        print(f"\nâ° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Enhanced Browser Scraping Solution")
    print("å¢å¼ºç‰ˆæµè§ˆå™¨æŠ“å–è§£å†³æ–¹æ¡ˆ")
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    scraper = EnhancedBrowserScraper()
    
    try:
        await scraper.run_comprehensive_scraping()
        scraper.display_results()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await scraper.cleanup()


if __name__ == "__main__":
    asyncio.run(main())