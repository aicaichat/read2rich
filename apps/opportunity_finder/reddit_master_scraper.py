#!/usr/bin/env python3
"""
Reddit ç»ˆææŠ“å–å™¨
ä¸“é—¨é’ˆå¯¹Redditä¼˜åŒ–çš„ä¸–ç•Œçº§æŠ“å–è§£å†³æ–¹æ¡ˆ
"""

import asyncio
import random
import json
import time
from datetime import datetime
from typing import List, Dict, Any
from playwright.async_api import async_playwright, Page


class RedditMasterScraper:
    """Redditç»ˆææŠ“å–å™¨ - ä¸“é—¨ç ´è§£Redditçš„åçˆ¬è™«æœºåˆ¶"""
    
    def __init__(self):
        self.browser = None
        self.context = None
        self.playwright = None
        self.session_id = f"reddit_{int(time.time())}"
    
    async def setup_reddit_optimized_browser(self):
        """è®¾ç½®ä¸“é—¨ä¼˜åŒ–çš„Redditæµè§ˆå™¨ç¯å¢ƒ"""
        print("ğŸ”¥ åˆå§‹åŒ–Redditç»ˆææŠ“å–ç¯å¢ƒ...")
        
        self.playwright = await async_playwright().start()
        
        # Redditä¸“ç”¨çš„ç”¨æˆ·ä»£ç†ï¼ˆé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«ï¼‰
        reddit_user_agent = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
        
        # å¯åŠ¨æµè§ˆå™¨
        self.browser = await self.playwright.chromium.launch(
            headless=False,
            slow_mo=1200,
            args=[
                '--start-maximized',
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage', 
                '--no-sandbox',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--enable-automation=false',
                '--disable-background-timer-throttling',
                '--disable-backgrounding-occluded-windows',
                '--disable-renderer-backgrounding',
                '--no-first-run',
                '--password-store=basic',
                '--use-mock-keychain',
                f'--user-agent={reddit_user_agent}'
            ]
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent=reddit_user_agent,
            locale='en-US',
            timezone_id='America/New_York',
            extra_http_headers={
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Cache-Control': 'max-age=0',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'sec-ch-ua': '"Google Chrome";v="121", "Not A(Brand";v="99", "Chromium";v="121"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-platform': '"macOS"'
            }
        )
        
        # æ³¨å…¥è¶…çº§åæ£€æµ‹è„šæœ¬
        await self.context.add_init_script("""
            console.log('ğŸ›¡ï¸ Redditåæ£€æµ‹ç³»ç»Ÿå·²æ¿€æ´»');
            
            // 1. å®Œå…¨ç§»é™¤webdriverç—•è¿¹
            Object.defineProperty(navigator, 'webdriver', {
                get: () => false,
            });
            
            // 2. è¦†ç›–User Agentç›¸å…³å±æ€§
            Object.defineProperty(navigator, 'languages', {
                get: () => ['en-US', 'en'],
            });
            
            // 3. æ¨¡æ‹ŸçœŸå®çš„æ’ä»¶åˆ—è¡¨
            Object.defineProperty(navigator, 'plugins', {
                get: () => [
                    {name: 'Chrome PDF Plugin', description: 'Portable Document Format', filename: 'internal-pdf-viewer'},
                    {name: 'Chrome PDF Viewer', description: '', filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai'},
                    {name: 'Native Client', description: '', filename: 'internal-nacl-plugin'}
                ],
            });
            
            // 4. è¦†ç›–æƒé™API
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
            );
            
            // 5. ç§»é™¤æ‰€æœ‰Playwright/Automationç›¸å…³çš„ç—•è¿¹
            delete window.playwright;
            delete window.__playwright;
            delete window._playwright;
            delete window.callPhantom;
            delete window._phantom;
            delete window.phantom;
            delete window.fmget_targets;
            delete window.spawn;
            delete window.emit;
            delete window.webdriver;
            delete window.domAutomation;
            delete window.domAutomationController;
            delete window.__webdriver_script_fn;
            delete window.__driver_evaluate;
            delete window.__webdriver_evaluate;
            delete window.__selenium_evaluate;
            delete window.__fxdriver_evaluate;
            delete window.__driver_unwrapped;
            delete window.__webdriver_unwrapped;
            delete window.__selenium_unwrapped;
            delete window.__fxdriver_unwrapped;
            
            // 6. è¦†ç›–console.debugä»¥éšè—è°ƒè¯•ä¿¡æ¯
            const originalDebug = console.debug;
            console.debug = function(...args) {
                if (args[0] && typeof args[0] === 'string' && 
                    (args[0].includes('playwright') || args[0].includes('automation'))) {
                    return;
                }
                return originalDebug.apply(console, args);
            };
            
            // 7. æ¨¡æ‹ŸçœŸå®çš„screenå±æ€§
            Object.defineProperty(screen, 'availHeight', {
                get: () => 1055,
            });
            
            // 8. æ¨¡æ‹Ÿç¡¬ä»¶ä¿¡æ¯
            Object.defineProperty(navigator, 'hardwareConcurrency', {
                get: () => 8,
            });
            
            Object.defineProperty(navigator, 'deviceMemory', {
                get: () => 8,
            });
            
            // 9. è¦†ç›–Dateæ—¶åŒº
            const originalGetTimezoneOffset = Date.prototype.getTimezoneOffset;
            Date.prototype.getTimezoneOffset = function() {
                return 300; // EST
            };
            
            // 10. æ·»åŠ çœŸå®çš„Chrome runtime
            if (!window.chrome) {
                window.chrome = {
                    runtime: {}
                };
            }
            
            console.log('âœ… Redditè¶…çº§åæ£€æµ‹è„šæœ¬é…ç½®å®Œæˆ');
        """)
        
        print("âœ… Redditä¸“ç”¨æµè§ˆå™¨ç¯å¢ƒé…ç½®å®Œæˆ")
    
    async def navigate_to_reddit_with_stealth(self, subreddit: str) -> Page:
        """ä½¿ç”¨éšèº«æŠ€æœ¯è®¿é—®Reddit"""
        page = await self.context.new_page()
        
        # æ·»åŠ é¡µé¢çº§åˆ«çš„åæ£€æµ‹
        await page.add_init_script("""
            // é¡µé¢çº§åˆ«çš„é¢å¤–ä¿æŠ¤
            Object.defineProperty(document, 'hidden', {
                get: () => false,
            });
            
            Object.defineProperty(document, 'visibilityState', {
                get: () => 'visible',
            });
        """)
        
        url = f"https://www.reddit.com/r/{subreddit}/"
        print(f"ğŸŒ éšèº«è®¿é—®: {url}")
        
        try:
            # é¦–å…ˆè®¿é—®Redditä¸»é¡µå»ºç«‹ä¼šè¯
            await page.goto("https://www.reddit.com/", wait_until='domcontentloaded', timeout=20000)
            await asyncio.sleep(random.uniform(2, 4))
            
            # ç„¶åè®¿é—®ç›®æ ‡subreddit
            await page.goto(url, wait_until='domcontentloaded', timeout=20000)
            await asyncio.sleep(random.uniform(3, 5))
            
            return page
            
        except Exception as e:
            print(f"âš ï¸ è®¿é—®å¤±è´¥: {e}")
            return page
    
    async def handle_reddit_popups_aggressively(self, page: Page):
        """æ¿€è¿›å¤„ç†Redditå¼¹çª—"""
        print("ğŸš« æ¿€è¿›å¤„ç†Redditå¼¹çª—å’Œè¦†ç›–å±‚...")
        
        # Redditç‰¹å®šçš„å¼¹çª—é€‰æ‹©å™¨
        reddit_popup_selectors = [
            # Appæ¨å¹¿å¼¹çª—
            '[data-testid="onboarding-close"]',
            '[data-testid="onboarding-dismiss"]',
            'button[aria-label="Close"]',
            
            # CookieåŒæ„
            'button:has-text("Accept all")',
            'button:has-text("Accept")',
            'button:has-text("Got it")',
            'button:has-text("OK")',
            
            # ç™»å½•å¼¹çª—
            'button:has-text("Maybe Later")',
            'button:has-text("Not now")',
            'button:has-text("Skip")',
            
            # Premiumæ¨å¹¿
            '[data-testid="premium-banner-close"]',
            '.premium-banner-close',
            
            # é€šç”¨å…³é—­æŒ‰é’®
            'button:has-text("Ã—")',
            'button:has-text("âœ•")',
            '[aria-label*="close"]',
            '[aria-label*="Close"]',
            '.close',
            '.dismiss',
            
            # ç‰¹å®šçš„Redditå¼¹çª—
            '[data-click-id="close"]',
            '[data-testid="close-button"]'
        ]
        
        for selector in reddit_popup_selectors:
            try:
                elements = await page.query_selector_all(selector)
                for element in elements:
                    if await element.is_visible():
                        await element.click()
                        print(f"âœ… å…³é—­å¼¹çª—: {selector}")
                        await asyncio.sleep(0.5)
            except:
                continue
        
        # å¤„ç†è¦†ç›–å±‚
        await page.evaluate("""
            // ç§»é™¤å¯èƒ½çš„è¦†ç›–å±‚
            const overlays = document.querySelectorAll('[style*="position: fixed"], [style*="z-index"]');
            overlays.forEach(overlay => {
                if (overlay.style.zIndex > 1000) {
                    overlay.remove();
                }
            });
            
            // ç¡®ä¿é¡µé¢å¯æ»šåŠ¨
            document.body.style.overflow = 'auto';
            document.documentElement.style.overflow = 'auto';
        """)
    
    async def smart_reddit_scroll_and_load(self, page: Page):
        """æ™ºèƒ½Redditæ»šåŠ¨å’Œå†…å®¹åŠ è½½"""
        print("ğŸ“œ æ‰§è¡Œæ™ºèƒ½Redditæ»šåŠ¨ç­–ç•¥...")
        
        # ç­‰å¾…åˆå§‹å†…å®¹åŠ è½½
        await asyncio.sleep(3)
        
        # æ‰§è¡Œå¤šè½®æ»šåŠ¨ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
        for round_num in range(4):
            print(f"   æ»šåŠ¨è½®æ¬¡ {round_num + 1}/4")
            
            # ç¼“æ…¢æ»šåŠ¨ï¼Œæ¨¡æ‹Ÿé˜…è¯»
            viewport_height = await page.evaluate("window.innerHeight")
            current_position = await page.evaluate("window.pageYOffset")
            target_position = current_position + viewport_height * 2
            
            # åˆ†æ®µæ»šåŠ¨
            steps = 5
            for step in range(steps):
                position = current_position + (target_position - current_position) * (step + 1) / steps
                await page.evaluate(f"window.scrollTo(0, {position})")
                await asyncio.sleep(random.uniform(0.8, 1.5))
            
            # ç­‰å¾…æ–°å†…å®¹åŠ è½½
            await asyncio.sleep(random.uniform(2, 4))
            
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾åº•éƒ¨
            is_at_bottom = await page.evaluate("""
                (window.innerHeight + window.pageYOffset) >= document.body.scrollHeight - 1000
            """)
            
            if is_at_bottom:
                print("   ğŸ“„ å·²æ¥è¿‘é¡µé¢åº•éƒ¨")
                break
    
    async def extract_reddit_posts_intelligently(self, page: Page, subreddit: str) -> List[Dict[str, Any]]:
        """æ™ºèƒ½æå–Redditå¸–å­"""
        print("ğŸ§  å¼€å§‹æ™ºèƒ½Redditæ•°æ®æå–...")
        
        items = []
        
        # Redditæ–°æ—§ç‰ˆæœ¬çš„å¤šé‡é€‰æ‹©å™¨ç­–ç•¥
        post_selectors = [
            # æ–°ç‰ˆReddit
            '[data-testid^="post-container"]',
            '[data-click-id="background"]',
            '[data-adclicklocation="title"]',
            
            # æ—§ç‰ˆReddit
            '.thing',
            '.link',
            
            # ç§»åŠ¨ç‰ˆå’Œå…¶ä»–ç‰ˆæœ¬
            'article',
            '.Post',
            '[role="article"]',
            
            # é€šç”¨å®¹å™¨
            'div[tabindex="0"]',
            'div[data-click-id]'
        ]
        
        posts = []
        for selector in post_selectors:
            posts = await page.query_selector_all(selector)
            if len(posts) > 5:  # æ‰¾åˆ°è¶³å¤Ÿçš„å¸–å­
                print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨æ‰¾åˆ° {len(posts)} ä¸ªå¸–å­: {selector}")
                break
        
        if not posts:
            print("âš ï¸ æœªæ‰¾åˆ°å¸–å­å®¹å™¨ï¼Œå°è¯•å…¨é¡µé¢æœç´¢...")
            # å¤‡ç”¨ç­–ç•¥ï¼šæœç´¢æ‰€æœ‰å¯èƒ½çš„æ ‡é¢˜å…ƒç´ 
            posts = await page.query_selector_all('h1, h2, h3, [role="heading"], .title, a[data-click-id="body"]')
            print(f"ğŸ” å¤‡ç”¨ç­–ç•¥æ‰¾åˆ° {len(posts)} ä¸ªå¯èƒ½çš„å¸–å­å…ƒç´ ")
        
        # æå–å¸–å­ä¿¡æ¯
        extracted_count = 0
        for i, post in enumerate(posts[:30]):  # é™åˆ¶å¤„ç†æ•°é‡
            try:
                # å¤šé‡æ ‡é¢˜æå–ç­–ç•¥
                title = await self._extract_reddit_title(post)
                if not title or len(title.strip()) < 10:
                    continue
                
                # æå–å…¶ä»–ä¿¡æ¯
                author = await self._extract_reddit_author(post)
                score = await self._extract_reddit_score(post)
                comments_count = await self._extract_reddit_comments(post)
                url = await self._extract_reddit_url(post, subreddit)
                
                item = {
                    'id': f"reddit_{subreddit}_{i}_{int(time.time())}",
                    'title': title.strip()[:300],
                    'author': author,
                    'score': score,
                    'comments_count': comments_count,
                    'url': url,
                    'subreddit': subreddit,
                    'platform': 'reddit',
                    'source': f'reddit_r_{subreddit}',
                    'scraped_at': datetime.now().isoformat(),
                    'method': 'reddit_master_scraper',
                    'session_id': self.session_id
                }
                
                items.append(item)
                extracted_count += 1
                print(f"  ğŸ“ {extracted_count:2d}. {title[:60]}...")
                
                if extracted_count >= 15:  # é™åˆ¶æå–æ•°é‡
                    break
                    
            except Exception as e:
                print(f"âš ï¸ æå–å¸–å­ {i} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"âœ… Redditæ™ºèƒ½æå–å®Œæˆ: {len(items)} æ¡æ•°æ®")
        return items
    
    async def _extract_reddit_title(self, post_element) -> str:
        """æå–Redditæ ‡é¢˜"""
        title_selectors = [
            'h3',
            '[slot="title"]',
            'a[data-click-id="body"]',
            '[data-adclicklocation="title"]',
            '.title a',
            '.Post-title',
            'h1', 'h2',
            '[role="heading"]',
            '.link-title'
        ]
        
        for selector in title_selectors:
            try:
                title_elem = await post_element.query_selector(selector)
                if title_elem:
                    title = await title_elem.text_content()
                    if title and len(title.strip()) > 5:
                        return title.strip()
            except:
                continue
        
        # å¤‡ç”¨ç­–ç•¥ï¼šç›´æ¥è·å–å…ƒç´ æ–‡æœ¬
        try:
            text = await post_element.text_content()
            if text and len(text.strip()) > 10:
                # å¦‚æœæ˜¯é•¿æ–‡æœ¬ï¼Œå–å‰100ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜
                return text.strip()[:100]
        except:
            pass
        
        return ""
    
    async def _extract_reddit_author(self, post_element) -> str:
        """æå–Redditä½œè€…"""
        author_selectors = [
            'a[data-testid="post_author_link"]',
            '[data-click-id="user"]',
            '.author',
            'a[href*="/user/"]',
            'a[href*="/u/"]'
        ]
        
        for selector in author_selectors:
            try:
                author_elem = await post_element.query_selector(selector)
                if author_elem:
                    author = await author_elem.text_content()
                    if author and author.strip():
                        return author.strip()
            except:
                continue
        return "Unknown"
    
    async def _extract_reddit_score(self, post_element) -> int:
        """æå–Redditè¯„åˆ†"""
        score_selectors = [
            '[data-testid="vote-arrows"] button span',
            '.score',
            '[aria-label*="upvote"]',
            '.upvotes'
        ]
        
        for selector in score_selectors:
            try:
                score_elem = await post_element.query_selector(selector)
                if score_elem:
                    score_text = await score_elem.text_content()
                    if score_text:
                        # è§£ææ•°å­—
                        import re
                        numbers = re.findall(r'\d+', score_text.replace(',', ''))
                        if numbers:
                            return int(numbers[0])
            except:
                continue
        return 0
    
    async def _extract_reddit_comments(self, post_element) -> int:
        """æå–Redditè¯„è®ºæ•°"""
        comment_selectors = [
            'a[data-click-id="comments"]',
            'a[href*="/comments/"]',
            '.comments',
            '[data-click-id="comments"] span'
        ]
        
        for selector in comment_selectors:
            try:
                comment_elem = await post_element.query_selector(selector)
                if comment_elem:
                    comment_text = await comment_elem.text_content()
                    if comment_text:
                        import re
                        numbers = re.findall(r'\d+', comment_text.replace(',', ''))
                        if numbers:
                            return int(numbers[0])
            except:
                continue
        return 0
    
    async def _extract_reddit_url(self, post_element, subreddit: str) -> str:
        """æå–Reddit URL"""
        url_selectors = [
            'a[data-click-id="body"]',
            'a[href*="/comments/"]',
            '.title a',
            'h3 a'
        ]
        
        for selector in url_selectors:
            try:
                url_elem = await post_element.query_selector(selector)
                if url_elem:
                    href = await url_elem.get_attribute('href')
                    if href:
                        if href.startswith('http'):
                            return href
                        elif href.startswith('/'):
                            return f"https://www.reddit.com{href}"
            except:
                continue
        
        return f"https://www.reddit.com/r/{subreddit}/"
    
    async def scrape_reddit_subreddit(self, subreddit: str) -> List[Dict[str, Any]]:
        """æŠ“å–Redditå­ç‰ˆå—"""
        print(f"\nğŸ¯ å¼€å§‹æŠ“å– r/{subreddit}")
        print("="*50)
        
        page = await self.navigate_to_reddit_with_stealth(subreddit)
        
        try:
            # ç­‰å¾…é¡µé¢ç¨³å®š
            await asyncio.sleep(3)
            
            # å¤„ç†å¼¹çª—
            await self.handle_reddit_popups_aggressively(page)
            
            # æ™ºèƒ½æ»šåŠ¨åŠ è½½
            await self.smart_reddit_scroll_and_load(page)
            
            # å†æ¬¡å¤„ç†å¯èƒ½å‡ºç°çš„å¼¹çª—
            await self.handle_reddit_popups_aggressively(page)
            
            # æ™ºèƒ½æ•°æ®æå–
            items = await self.extract_reddit_posts_intelligently(page, subreddit)
            
            return items
            
        except Exception as e:
            print(f"âŒ æŠ“å– r/{subreddit} å¤±è´¥: {e}")
            # ä¿å­˜è°ƒè¯•æˆªå›¾
            await page.screenshot(path=f'reddit_{subreddit}_error_{self.session_id}.png')
            return []
        
        finally:
            await page.close()
    
    async def run_reddit_master_scraping(self) -> List[Dict[str, Any]]:
        """è¿è¡ŒRedditç»ˆææŠ“å–"""
        print("ğŸ”¥ å¯åŠ¨Redditç»ˆææŠ“å–ç³»ç»Ÿ")
        print("="*60)
        
        await self.setup_reddit_optimized_browser()
        
        # è¦æŠ“å–çš„subredditåˆ—è¡¨
        subreddits = ['entrepreneur', 'startups', 'SaaS', 'smallbusiness', 'indiehackers']
        all_results = []
        
        for subreddit in subreddits[:3]:  # é™åˆ¶3ä¸ªä»¥é¿å…è¿‡é•¿
            try:
                items = await self.scrape_reddit_subreddit(subreddit)
                all_results.extend(items)
                
                # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«æ£€æµ‹
                delay = random.uniform(5, 10)
                print(f"â³ ç­‰å¾… {delay:.1f} ç§’åç»§ç»­ä¸‹ä¸€ä¸ªsubreddit...")
                await asyncio.sleep(delay)
                
            except Exception as e:
                print(f"âŒ æŠ“å– {subreddit} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        return all_results
    
    def analyze_results(self, results: List[Dict[str, Any]]):
        """åˆ†ææŠ“å–ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ‰ Redditç»ˆææŠ“å–å®Œæˆ!")
        print("="*60)
        
        if not results:
            print("âš ï¸ æœªè·å–åˆ°æ•°æ®")
            return
        
        # ç»Ÿè®¡åˆ†æ
        total = len(results)
        subreddits = {}
        quality_metrics = {
            'has_author': 0,
            'has_score': 0,
            'has_comments': 0,
            'has_url': 0
        }
        
        for item in results:
            subreddit = item.get('subreddit', 'unknown')
            subreddits[subreddit] = subreddits.get(subreddit, 0) + 1
            
            if item.get('author') and item['author'] != 'Unknown':
                quality_metrics['has_author'] += 1
            if item.get('score', 0) > 0:
                quality_metrics['has_score'] += 1
            if item.get('comments_count', 0) > 0:
                quality_metrics['has_comments'] += 1
            if item.get('url') and 'reddit.com' in item['url']:
                quality_metrics['has_url'] += 1
        
        print(f"ğŸ“Š æ€»æ•°æ®é‡: {total} æ¡")
        print(f"ğŸ“ˆ æ•°æ®è´¨é‡æŒ‡æ ‡:")
        for metric, count in quality_metrics.items():
            percentage = (count / total) * 100 if total > 0 else 0
            print(f"   {metric}: {count}/{total} ({percentage:.1f}%)")
        
        print(f"\nğŸŒ Subredditåˆ†å¸ƒ:")
        for subreddit, count in sorted(subreddits.items()):
            percentage = (count / total) * 100
            print(f"   r/{subreddit:15}: {count:3d} æ¡ ({percentage:5.1f}%)")
        
        print(f"\nğŸ“ é«˜è´¨é‡æ•°æ®æ ·æœ¬:")
        # æ˜¾ç¤ºè´¨é‡æœ€é«˜çš„å¸–å­
        sorted_results = sorted(results, key=lambda x: (
            len(x.get('title', '')),
            x.get('score', 0),
            x.get('comments_count', 0)
        ), reverse=True)
        
        for i, item in enumerate(sorted_results[:8]):
            title = item.get('title', 'No title')[:50]
            author = item.get('author', 'Unknown')
            score = item.get('score', 0)
            comments = item.get('comments_count', 0)
            subreddit = item.get('subreddit', '')
            
            print(f"  {i+1}. [r/{subreddit}] {title}...")
            print(f"      ğŸ‘¤ {author} | â­ {score} | ğŸ’¬ {comments}")
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"reddit_master_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                'session_id': self.session_id,
                'timestamp': timestamp,
                'total_items': total,
                'quality_metrics': quality_metrics,
                'subreddits': subreddits,
                'data': results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        print(f"â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        print("\nğŸ§¹ æ¸…ç†Redditç»ˆææŠ“å–å™¨èµ„æº...")
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        print("âœ… æ¸…ç†å®Œæˆ")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¥ Redditç»ˆææŠ“å–å™¨")
    print("ğŸ•·ï¸ ä¸“é—¨ç ´è§£Redditåçˆ¬è™«æœºåˆ¶")
    print(f"â° å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    scraper = RedditMasterScraper()
    
    try:
        results = await scraper.run_reddit_master_scraping()
        scraper.analyze_results(results)
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await scraper.cleanup()


if __name__ == "__main__":
    asyncio.run(main())