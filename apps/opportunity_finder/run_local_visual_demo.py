#!/usr/bin/env python3
"""
æœ¬åœ°å¯è§†åŒ–æµè§ˆå™¨æŠ“å–æ¼”ç¤º
ç›´æ¥åœ¨æœ¬åœ°ç¯å¢ƒè¿è¡Œï¼Œæ— éœ€Dockerå®¹å™¨
"""

import asyncio
import sys
import os
import signal
from datetime import datetime
import json

async def install_playwright_if_needed():
    """æ£€æŸ¥å¹¶å®‰è£…Playwright"""
    try:
        import playwright
        print("âœ… Playwrightå·²å®‰è£…")
        return True
    except ImportError:
        print("ğŸ“¦ Playwrightæœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
        try:
            import subprocess
            subprocess.check_call([sys.executable, "-m", "pip", "install", "playwright", "--break-system-packages"])
            subprocess.check_call([sys.executable, "-m", "playwright", "install", "chromium"])
            print("âœ… Playwrightå®‰è£…å®Œæˆ")
            return True
        except Exception as e:
            print(f"âŒ Playwrightå®‰è£…å¤±è´¥: {e}")
            return False

async def run_visual_scraping_demo():
    """è¿è¡Œå¯è§†åŒ–æŠ“å–æ¼”ç¤º"""
    
    # Check playwright installation
    if not await install_playwright_if_needed():
        return
    
    # Import after installation
    from playwright.async_api import async_playwright
    import random
    
    print("ğŸ­ å¼€å§‹å¯è§†åŒ–æµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ“å–æ¼”ç¤º")
    print("=" * 60)
    print("âš ï¸  æ³¨æ„: æµè§ˆå™¨çª—å£å°†ä¼šè‡ªåŠ¨æ‰“å¼€ï¼Œè¯·ä¸è¦å…³é—­!")
    print("ğŸ¯ æ¼”ç¤ºå°†ä¾æ¬¡è®¿é—®: Reddit, HackerNews, Product Hunt")
    print("â±ï¸  æ•´ä¸ªè¿‡ç¨‹å¤§çº¦éœ€è¦ 2-3 åˆ†é’Ÿ")
    print("=" * 60)
    
    await asyncio.sleep(3)
    
    playwright = await async_playwright().start()
    all_items = []
    
    try:
        # Launch visible browser
        browser = await playwright.chromium.launch(
            headless=False,
            slow_mo=2000,  # 2 second delay between actions
            args=['--start-maximized']
        )
        
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
        )
        
        page = await context.new_page()
        
        # 1. Scrape Reddit
        print("\nğŸ“± æ­£åœ¨å¯è§†åŒ–æŠ“å– Reddit r/entrepreneur...")
        try:
            await page.goto("https://www.reddit.com/r/entrepreneur/", wait_until='networkidle')
            await asyncio.sleep(3)  # Visual pause
            
            # Try to find posts
            posts = await page.query_selector_all('h3')
            reddit_items = []
            
            for i, post in enumerate(posts[:5]):
                try:
                    title = await post.text_content()
                    if title and len(title) > 10:
                        item = {
                            'id': f"reddit_visual_{i}",
                            'title': title.strip(),
                            'source': 'reddit_entrepreneur',
                            'platform': 'reddit',
                            'scraped_at': datetime.now().isoformat()
                        }
                        reddit_items.append(item)
                        print(f"  ğŸ“ å‘ç°å¸–å­: {title[:50]}...")
                except:
                    continue
            
            all_items.extend(reddit_items)
            print(f"âœ… RedditæŠ“å–å®Œæˆ: {len(reddit_items)} æ¡æ•°æ®")
            
        except Exception as e:
            print(f"âŒ RedditæŠ“å–å¤±è´¥: {e}")
        
        await asyncio.sleep(3)  # Visual pause
        
        # 2. Scrape Hacker News
        print("\nğŸ“° æ­£åœ¨å¯è§†åŒ–æŠ“å– Hacker News...")
        try:
            await page.goto("https://news.ycombinator.com/", wait_until='networkidle')
            await asyncio.sleep(3)  # Visual pause
            
            # Find story titles
            stories = await page.query_selector_all('span.titleline > a')
            hn_items = []
            
            for i, story in enumerate(stories[:5]):
                try:
                    title = await story.text_content()
                    href = await story.get_attribute('href')
                    
                    if title:
                        item = {
                            'id': f"hn_visual_{i}",
                            'title': title.strip(),
                            'url': href,
                            'source': 'hackernews',
                            'platform': 'hackernews',
                            'scraped_at': datetime.now().isoformat()
                        }
                        hn_items.append(item)
                        print(f"  ğŸ“ å‘ç°æ•…äº‹: {title[:50]}...")
                except:
                    continue
            
            all_items.extend(hn_items)
            print(f"âœ… HackerNewsæŠ“å–å®Œæˆ: {len(hn_items)} æ¡æ•°æ®")
            
        except Exception as e:
            print(f"âŒ HackerNewsæŠ“å–å¤±è´¥: {e}")
        
        await asyncio.sleep(3)  # Visual pause
        
        # 3. Scrape Product Hunt
        print("\nğŸš€ æ­£åœ¨å¯è§†åŒ–æŠ“å– Product Hunt...")
        try:
            await page.goto("https://www.producthunt.com/", wait_until='networkidle')
            await asyncio.sleep(5)  # Longer pause for Product Hunt
            
            # Find products
            products = await page.query_selector_all('h3')
            ph_items = []
            
            for i, product in enumerate(products[:5]):
                try:
                    title = await product.text_content()
                    
                    if title and len(title) > 5:
                        item = {
                            'id': f"ph_visual_{i}",
                            'title': title.strip(),
                            'source': 'product_hunt',
                            'platform': 'product_hunt',
                            'scraped_at': datetime.now().isoformat()
                        }
                        ph_items.append(item)
                        print(f"  ğŸ“ å‘ç°äº§å“: {title[:50]}...")
                except:
                    continue
            
            all_items.extend(ph_items)
            print(f"âœ… Product HuntæŠ“å–å®Œæˆ: {len(ph_items)} æ¡æ•°æ®")
            
        except Exception as e:
            print(f"âŒ Product HuntæŠ“å–å¤±è´¥: {e}")
        
        # Visual completion pause
        print("\nğŸ‰ æŠ“å–æ¼”ç¤ºå³å°†å®Œæˆ...")
        await asyncio.sleep(3)
        
        await context.close()
        await browser.close()
        
    except Exception as e:
        print(f"âŒ æµè§ˆå™¨æ“ä½œå¤±è´¥: {e}")
    
    finally:
        await playwright.stop()
    
    # Display results
    print("\nğŸ‰ å¯è§†åŒ–æŠ“å–æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)
    print(f"ğŸ“Š æ€»å…±è·å–æ•°æ®: {len(all_items)} æ¡")
    
    if all_items:
        print("\nğŸ“ å®Œæ•´æ•°æ®æ ·æœ¬:")
        for i, item in enumerate(all_items):
            platform = item.get('platform', 'unknown')
            title = item.get('title', 'No title')[:60]
            print(f"  {i+1:2d}. [{platform.upper():12}] {title}...")
        
        # Group by platform
        platforms = {}
        for item in all_items:
            platform = item.get('platform', 'unknown')
            platforms[platform] = platforms.get(platform, 0) + 1
        
        print(f"\nğŸ“ˆ æ•°æ®æºåˆ†å¸ƒ:")
        for platform, count in platforms.items():
            print(f"  ğŸŒ {platform:15}: {count} æ¡")
        
        # Save to file
        output_file = f"visual_scraping_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_items, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    print(f"\nâ° æ¼”ç¤ºå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ AI Opportunity Finder æœ¬åœ°å¯è§†åŒ–æŠ“å–æ¼”ç¤º")
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    try:
        await run_visual_scraping_demo()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ¼”ç¤º")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("ğŸ¯ æœ¬åœ°å¯è§†åŒ–æŠ“å–æ¼”ç¤º")
    print("ğŸ–¥ï¸  æ­¤æ¼”ç¤ºå°†åœ¨æ‚¨çš„æœ¬åœ°ç¯å¢ƒè¿è¡Œæµè§ˆå™¨")
    print("ğŸ“± æ‚¨å°†çœ‹åˆ°æµè§ˆå™¨è‡ªåŠ¨è®¿é—®å„ä¸ªç½‘ç«™å¹¶æŠ“å–æ•°æ®")
    print()
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¼”ç¤ºå·²é€€å‡º")
    except Exception as e:
        print(f"\nğŸ’¥ å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)