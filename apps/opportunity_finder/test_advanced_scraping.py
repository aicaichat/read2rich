#!/usr/bin/env python3
"""
Test script for advanced scraping methods.
Tests browser automation and smart HTTP scraping.
"""

import asyncio
import sys
import os
import json
from datetime import datetime
from loguru import logger

# Add the ingestion_service to Python path
sys.path.append(os.path.join(os.path.dirname(__file__), 'ingestion_service'))

from config import Settings
from scrapers.smart_http_scraper import SmartHttpScraper
from scrapers.browser_scraper import BrowserScraper


async def test_smart_http_scraping():
    """Test the smart HTTP scraper."""
    print("\nğŸ§  æµ‹è¯•æ™ºèƒ½HTTPæŠ“å–å™¨...")
    print("=" * 50)
    
    try:
        settings = Settings()
        scraper = SmartHttpScraper(settings)
        
        # Test Reddit scraping
        print("ğŸ“± æµ‹è¯•RedditæŠ“å–...")
        reddit_items = await scraper._scrape_reddit_smart('entrepreneur')
        print(f"âœ… RedditæˆåŠŸæŠ“å–: {len(reddit_items)} æ¡æ•°æ®")
        
        if reddit_items:
            print("ğŸ“ Redditç¤ºä¾‹æ•°æ®:")
            sample = reddit_items[0]
            print(f"   æ ‡é¢˜: {sample.get('title', '')[:60]}...")
            print(f"   è¯„åˆ†: {sample.get('score', 0)}")
            print(f"   ä½œè€…: {sample.get('author', '')}")
        
        # Test HackerNews scraping
        print("\nğŸ“° æµ‹è¯•HackerNewsæŠ“å–...")
        hn_items = await scraper._scrape_hackernews_smart()
        print(f"âœ… HackerNewsæˆåŠŸæŠ“å–: {len(hn_items)} æ¡æ•°æ®")
        
        if hn_items:
            print("ğŸ“ HackerNewsç¤ºä¾‹æ•°æ®:")
            sample = hn_items[0]
            print(f"   æ ‡é¢˜: {sample.get('title', '')[:60]}...")
            print(f"   ç§¯åˆ†: {sample.get('points', 0)}")
            print(f"   è¯„è®º: {sample.get('num_comments', 0)}")
        
        total_items = len(reddit_items) + len(hn_items)
        print(f"\nğŸ‰ æ™ºèƒ½HTTPæŠ“å–å®Œæˆ! æ€»å…±è·å–: {total_items} æ¡æ•°æ®")
        
        return reddit_items + hn_items
        
    except Exception as e:
        print(f"âŒ æ™ºèƒ½HTTPæŠ“å–é”™è¯¯: {e}")
        logger.exception("Smart HTTP scraping error")
        return []


async def test_browser_scraping():
    """Test the browser automation scraper."""
    print("\nğŸŒ æµ‹è¯•æµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ“å–å™¨...")
    print("=" * 50)
    
    try:
        settings = Settings()
        scraper = BrowserScraper(settings)
        
        # Test Reddit browser scraping
        print("ğŸ“± æµ‹è¯•æµè§ˆå™¨RedditæŠ“å–...")
        reddit_items = await scraper._scrape_reddit_with_browser('startups')
        print(f"âœ… æµè§ˆå™¨RedditæˆåŠŸæŠ“å–: {len(reddit_items)} æ¡æ•°æ®")
        
        if reddit_items:
            print("ğŸ“ æµè§ˆå™¨Redditç¤ºä¾‹æ•°æ®:")
            sample = reddit_items[0]
            print(f"   æ ‡é¢˜: {sample.get('title', '')[:60]}...")
            print(f"   è¯„åˆ†: {sample.get('score', 0)}")
            print(f"   ä½œè€…: {sample.get('author', '')}")
        
        # Test HackerNews browser scraping
        print("\nğŸ“° æµ‹è¯•æµè§ˆå™¨HackerNewsæŠ“å–...")
        hn_items = await scraper._scrape_hackernews_with_browser()
        print(f"âœ… æµè§ˆå™¨HackerNewsæˆåŠŸæŠ“å–: {len(hn_items)} æ¡æ•°æ®")
        
        if hn_items:
            print("ğŸ“ æµè§ˆå™¨HackerNewsç¤ºä¾‹æ•°æ®:")
            sample = hn_items[0]
            print(f"   æ ‡é¢˜: {sample.get('title', '')[:60]}...")
            print(f"   Story ID: {sample.get('story_id', '')}")
        
        total_items = len(reddit_items) + len(hn_items)
        print(f"\nğŸ‰ æµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ“å–å®Œæˆ! æ€»å…±è·å–: {total_items} æ¡æ•°æ®")
        
        # Cleanup browser resources
        await scraper.cleanup()
        
        return reddit_items + hn_items
        
    except Exception as e:
        print(f"âŒ æµè§ˆå™¨æŠ“å–é”™è¯¯: {e}")
        logger.exception("Browser scraping error")
        return []


async def compare_scraping_methods():
    """Compare different scraping methods."""
    print("\nğŸ“Š æŠ“å–æ–¹æ³•å¯¹æ¯”æµ‹è¯•")
    print("=" * 50)
    
    results = {}
    
    # Test smart HTTP scraping
    start_time = datetime.now()
    smart_items = await test_smart_http_scraping()
    smart_duration = (datetime.now() - start_time).total_seconds()
    results['smart_http'] = {
        'items': len(smart_items),
        'duration': smart_duration,
        'success_rate': (len(smart_items) / 50) * 100 if smart_items else 0  # Assuming target of 50 items
    }
    
    # Test browser scraping
    start_time = datetime.now()
    browser_items = await test_browser_scraping()
    browser_duration = (datetime.now() - start_time).total_seconds()
    results['browser'] = {
        'items': len(browser_items),
        'duration': browser_duration,
        'success_rate': (len(browser_items) / 35) * 100 if browser_items else 0  # Assuming target of 35 items
    }
    
    # Print comparison
    print("\nğŸ“‹ æŠ“å–æ–¹æ³•å¯¹æ¯”ç»“æœ:")
    print("-" * 50)
    print(f"{'æ–¹æ³•':<15} {'æ•°æ®é‡':<8} {'è€—æ—¶(ç§’)':<10} {'æˆåŠŸç‡':<8}")
    print("-" * 50)
    
    for method, data in results.items():
        print(f"{method:<15} {data['items']:<8} {data['duration']:<10.1f} {data['success_rate']:<8.1f}%")
    
    # Save results to file
    with open('scraping_test_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'results': results,
            'smart_items_sample': smart_items[:3] if smart_items else [],
            'browser_items_sample': browser_items[:3] if browser_items else []
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: scraping_test_results.json")
    
    # Recommendations
    print("\nğŸ’¡ å»ºè®®:")
    if results['smart_http']['items'] > results['browser']['items']:
        print("   âœ… æ™ºèƒ½HTTPæŠ“å–å™¨è¡¨ç°æ›´å¥½ï¼Œæ¨èä¼˜å…ˆä½¿ç”¨")
    elif results['browser']['items'] > results['smart_http']['items']:
        print("   âœ… æµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ“å–å™¨è¡¨ç°æ›´å¥½ï¼Œæ¨èä¼˜å…ˆä½¿ç”¨")
    else:
        print("   âš–ï¸  ä¸¤ç§æ–¹æ³•è¡¨ç°ç›¸å½“ï¼Œå¯ä»¥å¹¶è¡Œä½¿ç”¨æé«˜è¦†ç›–ç‡")
    
    return results


async def main():
    """Main test function."""
    print("ğŸš€ AI Opportunity Finder é«˜çº§æŠ“å–æµ‹è¯•")
    print("=" * 60)
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # Run comparison test
        results = await compare_scraping_methods()
        
        print("\nğŸ¯ æµ‹è¯•æ€»ç»“:")
        total_items = sum(data['items'] for data in results.values())
        total_duration = sum(data['duration'] for data in results.values())
        
        print(f"   ğŸ“Š æ€»æ•°æ®é‡: {total_items} æ¡")
        print(f"   â±ï¸  æ€»è€—æ—¶: {total_duration:.1f} ç§’")
        print(f"   ğŸ“ˆ å¹³å‡é€Ÿåº¦: {total_items/total_duration:.1f} æ¡/ç§’")
        
        if total_items > 30:
            print("   âœ… æŠ“å–æµ‹è¯•æˆåŠŸ! é«˜çº§æŠ“å–å™¨å·¥ä½œæ­£å¸¸")
        elif total_items > 10:
            print("   âš ï¸  æŠ“å–æµ‹è¯•éƒ¨åˆ†æˆåŠŸï¼Œå¯èƒ½å­˜åœ¨ç½‘ç»œé™åˆ¶")
        else:
            print("   âŒ æŠ“å–æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥ç½‘ç»œå’Œé…ç½®")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        logger.exception("Test error")


if __name__ == "__main__":
    # Configure logging
    logger.remove()
    logger.add(sys.stdout, level="INFO", format="{time:HH:mm:ss} | {level} | {message}")
    
    # Run tests
    asyncio.run(main())