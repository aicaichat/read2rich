#!/usr/bin/env python3
"""
ç»ˆææµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ“å–ç³»ç»Ÿ
ä¸–ç•Œçº§çˆ¬è™«ä¸“å®¶è®¾è®¡çš„é«˜çº§æŠ“å–è§£å†³æ–¹æ¡ˆ
"""

import asyncio
import random
import json
import time
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from playwright.async_api import async_playwright, Page, BrowserContext
import re


@dataclass
class ScrapingTarget:
    """æŠ“å–ç›®æ ‡é…ç½®"""
    name: str
    url: str
    selectors: Dict[str, List[str]]  # å¤šé‡é€‰æ‹©å™¨ç­–ç•¥
    wait_conditions: List[str]
    scroll_strategy: str = "default"
    anti_detection: Dict[str, Any] = None


class WorldClassBrowserScraper:
    """ä¸–ç•Œçº§æµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ“å–å™¨"""
    
    def __init__(self):
        self.browser = None
        self.context = None
        self.playwright = None
        self.results = []
        self.session_id = self._generate_session_id()
        
        # é«˜çº§ç”¨æˆ·ä»£ç†æ± 
        self.user_agents = [
            # Chrome - Windows
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            # Chrome - macOS  
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            # Firefox
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0",
            # Safari
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15",
        ]
        
        # è§†çª—å¤§å°æ± 
        self.viewports = [
            {"width": 1920, "height": 1080},
            {"width": 1366, "height": 768}, 
            {"width": 1440, "height": 900},
            {"width": 1536, "height": 864},
            {"width": 1280, "height": 720}
        ]
        
        # æŠ“å–ç›®æ ‡é…ç½®
        self.targets = {
            'reddit': ScrapingTarget(
                name="Reddit",
                url="https://www.reddit.com/r/{}/",
                selectors={
                    'posts': [
                        '[data-testid="post-container"]',
                        'article',
                        'div[data-click-id="body"]',
                        '.Post',
                        '[data-adclicklocation="title"]'
                    ],
                    'title': [
                        'h3',
                        '[slot="title"]', 
                        'a[data-click-id="body"]',
                        '.title a',
                        '[data-adclicklocation="title"]'
                    ],
                    'score': [
                        '[data-testid="vote-arrows"] button span',
                        '.score',
                        '[aria-label*="upvote"]'
                    ],
                    'author': [
                        'a[data-testid="post_author_link"]',
                        '.author',
                        '[data-click-id="user"]'
                    ],
                    'comments': [
                        'a[data-click-id="comments"]',
                        '[data-click-id="comments"] span'
                    ]
                },
                wait_conditions=['networkidle', '[data-testid="post-container"], article, .Post'],
                scroll_strategy="infinite",
                anti_detection={
                    'close_popups': True,
                    'random_mouse_movement': True,
                    'typing_delay': True
                }
            ),
            
            'hackernews': ScrapingTarget(
                name="HackerNews", 
                url="https://news.ycombinator.com/",
                selectors={
                    'posts': [
                        'tr.athing',
                        '.storylink'
                    ],
                    'title': [
                        'span.titleline > a',
                        '.storylink'
                    ],
                    'score': [
                        '.score',
                        'span[id^="score_"]'
                    ],
                    'comments': [
                        'a[href*="item?id="]'
                    ],
                    'domain': [
                        '.sitestr'
                    ]
                },
                wait_conditions=['networkidle', 'tr.athing'],
                scroll_strategy="none"
            ),
            
            'producthunt': ScrapingTarget(
                name="Product Hunt",
                url="https://www.producthunt.com/",
                selectors={
                    'posts': [
                        '[data-test*="product"]',
                        'article',
                        '.item',
                        'div[style*="cursor: pointer"]'
                    ],
                    'title': [
                        'h3',
                        'h2', 
                        '[data-test="product-name"]',
                        'strong'
                    ],
                    'description': [
                        'p',
                        '[data-test="product-description"]',
                        '.description'
                    ],
                    'votes': [
                        '[data-test="vote-button"]',
                        '.vote-count',
                        'button[aria-label*="upvote"]'
                    ]
                },
                wait_conditions=['networkidle', 'h3, h2, article'],
                scroll_strategy="smooth",
                anti_detection={
                    'wait_for_js': 5,
                    'random_scroll': True
                }
            ),
            
            'indiehackers': ScrapingTarget(
                name="IndieHackers",
                url="https://www.indiehackers.com/",
                selectors={
                    'posts': [
                        '.feed-item',
                        'article',
                        '.post-item'
                    ],
                    'title': [
                        'h2 a',
                        'h3 a',
                        '.post-title'
                    ],
                    'author': [
                        '.author',
                        '[data-test="username"]'
                    ],
                    'engagement': [
                        '.engagement',
                        '.stats'
                    ]
                },
                wait_conditions=['networkidle', '.feed-item, article'],
                scroll_strategy="smooth"
            )
        }
    
    def _generate_session_id(self) -> str:
        """ç”Ÿæˆä¼šè¯ID"""
        return hashlib.md5(str(time.time()).encode()).hexdigest()[:12]
    
    async def setup_ultimate_browser(self) -> None:
        """è®¾ç½®ç»ˆææµè§ˆå™¨ç¯å¢ƒ"""
        print("ğŸš€ åˆå§‹åŒ–ä¸–ç•Œçº§æµè§ˆå™¨è‡ªåŠ¨åŒ–ç¯å¢ƒ...")
        
        self.playwright = await async_playwright().start()
        
        # éšæœºé€‰æ‹©ç”¨æˆ·ä»£ç†å’Œè§†çª—
        user_agent = random.choice(self.user_agents)
        viewport = random.choice(self.viewports)
        
        print(f"ğŸ­ ä½¿ç”¨ç”¨æˆ·ä»£ç†: {user_agent[:50]}...")
        print(f"ğŸ“± è§†çª—å¤§å°: {viewport['width']}x{viewport['height']}")
        
        # é«˜çº§æµè§ˆå™¨é…ç½®
        self.browser = await self.playwright.chromium.launch(
            headless=False,  # å¯è§†åŒ–æ¨¡å¼
            slow_mo=800,     # é€‚ä¸­çš„å»¶è¿Ÿ
            args=[
                '--start-maximized',
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--disable-background-timer-throttling',
                '--disable-backgrounding-occluded-windows',
                '--disable-renderer-backgrounding',
                '--disable-field-trial-config',
                '--disable-back-forward-cache',
                '--disable-ipc-flooding-protection',
                '--enable-automation=false',
                '--no-first-run',
                '--password-store=basic',
                '--use-mock-keychain',
                f'--user-agent={user_agent}'
            ]
        )
        
        # åˆ›å»ºéšèº«ä¸Šä¸‹æ–‡
        self.context = await self.browser.new_context(
            viewport=viewport,
            user_agent=user_agent,
            locale='en-US',
            timezone_id='America/New_York',
            permissions=['geolocation'],
            color_scheme='light',
            reduced_motion='reduce',
            extra_http_headers={
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Cache-Control': 'max-age=0',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1'
            }
        )
        
        # æ³¨å…¥ä¸–ç•Œçº§åæ£€æµ‹è„šæœ¬
        await self.context.add_init_script("""
            console.log('ğŸ›¡ï¸ ä¸–ç•Œçº§åæ£€æµ‹ç³»ç»Ÿå·²æ¿€æ´»');
            
            // 1. ç§»é™¤webdriveræ ‡è¯†
            Object.defineProperty(navigator, 'webdriver', {
                get: () => false,
            });
            
            // 2. æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ç‰¹å¾
            Object.defineProperty(navigator, 'languages', {
                get: () => ['en-US', 'en', 'zh-CN', 'zh'],
            });
            
            Object.defineProperty(navigator, 'plugins', {
                get: () => [
                    {name: 'Chrome PDF Plugin', description: 'Portable Document Format'},
                    {name: 'Chrome PDF Viewer', description: 'PDF Viewer'},
                    {name: 'Native Client', description: 'Native Client'},
                    {name: 'Chromium PDF Plugin', description: 'Portable Document Format'},
                    {name: 'Microsoft Edge PDF Plugin', description: 'PDF Plugin'}
                ],
            });
            
            // 3. è¦†ç›–Permission API
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
            );
            
            // 4. æ¨¡æ‹ŸçœŸå®çš„screenå±æ€§
            Object.defineProperty(screen, 'availHeight', {
                get: () => window.screen.height - 40,
            });
            
            Object.defineProperty(screen, 'availWidth', {
                get: () => window.screen.width,
            });
            
            // 5. è¦†ç›–Date.prototype.getTimezoneOffset
            const originalGetTimezoneOffset = Date.prototype.getTimezoneOffset;
            Date.prototype.getTimezoneOffset = function() {
                return 300; // EST timezone
            };
            
            // 6. æ¨¡æ‹ŸçœŸå®çš„hardwareConcurrency
            Object.defineProperty(navigator, 'hardwareConcurrency', {
                get: () => 8,
            });
            
            // 7. æ¨¡æ‹ŸdeviceMemory
            Object.defineProperty(navigator, 'deviceMemory', {
                get: () => 8,
            });
            
            // 8. ç§»é™¤Playwrightç‰¹å¾
            delete window.playwright;
            delete window.__playwright;
            delete window._playwright;
            
            // 9. è¦†ç›–console.debug
            const originalDebug = console.debug;
            console.debug = function(...args) {
                if (args[0] && args[0].includes && args[0].includes('playwright')) {
                    return;
                }
                return originalDebug.apply(console, args);
            };
            
            console.log('âœ… åæ£€æµ‹è„šæœ¬é…ç½®å®Œæˆ');
        """)
        
        print("âœ… ä¸–ç•Œçº§æµè§ˆå™¨ç¯å¢ƒè®¾ç½®å®Œæˆ")
    
    async def smart_wait_and_load(self, page: Page, target: ScrapingTarget) -> None:
        """æ™ºèƒ½ç­‰å¾…å’ŒåŠ è½½ç­–ç•¥"""
        print(f"â³ æ™ºèƒ½ç­‰å¾…é¡µé¢åŠ è½½...")
        
        # 1. åŸºç¡€ç­‰å¾…
        try:
            await page.wait_for_load_state('networkidle', timeout=15000)
        except:
            print("âš ï¸ ç½‘ç»œç©ºé—²ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­...")
        
        # 2. å…ƒç´ ç­‰å¾…
        for condition in target.wait_conditions:
            if condition != 'networkidle':
                try:
                    await page.wait_for_selector(condition, timeout=8000)
                    print(f"âœ… æ‰¾åˆ°å…ƒç´ : {condition}")
                    break
                except:
                    print(f"âš ï¸ å…ƒç´ ç­‰å¾…è¶…æ—¶: {condition}")
                    continue
        
        # 3. JavaScriptæ‰§è¡Œç­‰å¾…
        if target.anti_detection and target.anti_detection.get('wait_for_js'):
            await asyncio.sleep(target.anti_detection['wait_for_js'])
        
        # 4. æ™ºèƒ½æ»šåŠ¨ç­–ç•¥
        await self._execute_scroll_strategy(page, target.scroll_strategy)
    
    async def _execute_scroll_strategy(self, page: Page, strategy: str) -> None:
        """æ‰§è¡Œæ™ºèƒ½æ»šåŠ¨ç­–ç•¥"""
        if strategy == "infinite":
            print("ğŸ“œ æ‰§è¡Œæ— é™æ»šåŠ¨ç­–ç•¥...")
            for i in range(5):
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(random.uniform(2, 4))
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹åŠ è½½
                old_height = await page.evaluate("document.body.scrollHeight")
                await asyncio.sleep(2)
                new_height = await page.evaluate("document.body.scrollHeight")
                
                if old_height == new_height:
                    print("ğŸ“„ å·²åˆ°è¾¾é¡µé¢åº•éƒ¨")
                    break
        
        elif strategy == "smooth":
            print("ğŸŒŠ æ‰§è¡Œå¹³æ»‘æ»šåŠ¨ç­–ç•¥...")
            viewport_height = await page.evaluate("window.innerHeight")
            total_height = await page.evaluate("document.body.scrollHeight")
            
            for position in range(0, total_height, viewport_height // 2):
                await page.evaluate(f"window.scrollTo(0, {position})")
                await asyncio.sleep(random.uniform(1, 2))
    
    async def handle_popups_and_overlays(self, page: Page) -> None:
        """å¤„ç†å¼¹çª—å’Œè¦†ç›–å±‚"""
        print("ğŸš« æ£€æŸ¥å¹¶å…³é—­å¼¹çª—...")
        
        popup_selectors = [
            # é€šç”¨å…³é—­æŒ‰é’®
            '[aria-label="Close"]',
            '[aria-label="close"]', 
            'button:has-text("Close")',
            'button:has-text("Ã—")',
            'button:has-text("âœ•")',
            '[data-testid="close-button"]',
            '.close-button',
            '.modal-close',
            
            # Redditç‰¹å®š
            '[data-testid="onboarding-close"]',
            '[data-testid="premium-banner-close"]',
            'button[aria-label="Close"]',
            
            # Product Huntç‰¹å®š
            '[data-test="dismiss-button"]',
            '.dismiss',
            
            # Cookieæ¨ªå¹…
            'button:has-text("Accept")',
            'button:has-text("Got it")',
            'button:has-text("OK")',
            '.cookie-accept'
        ]
        
        for selector in popup_selectors:
            try:
                element = await page.query_selector(selector)
                if element:
                    await element.click()
                    print(f"âœ… å…³é—­å¼¹çª—: {selector}")
                    await asyncio.sleep(0.5)
            except:
                continue
    
    async def smart_extract_data(self, page: Page, target: ScrapingTarget) -> List[Dict[str, Any]]:
        """æ™ºèƒ½æ•°æ®æå–"""
        print(f"ğŸ§  å¼€å§‹æ™ºèƒ½æ•°æ®æå–...")
        
        items = []
        
        # 1. å¯»æ‰¾ä¸»è¦å†…å®¹å®¹å™¨
        posts = []
        for selector_list in target.selectors['posts']:
            posts = await page.query_selector_all(selector_list)
            if posts and len(posts) > 3:  # è‡³å°‘æ‰¾åˆ°3ä¸ªä»¥ä¸Šæ‰è®¤ä¸ºæœ‰æ•ˆ
                print(f"âœ… æ‰¾åˆ° {len(posts)} ä¸ªå†…å®¹é¡¹ (é€‰æ‹©å™¨: {selector_list})")
                break
        
        if not posts:
            print("âš ï¸ æœªæ‰¾åˆ°å†…å®¹é¡¹ï¼Œå°è¯•å¤‡ç”¨ç­–ç•¥...")
            await page.screenshot(path=f'{target.name.lower()}_debug_{self.session_id}.png')
            return items
        
        # 2. æ™ºèƒ½æå–æ¯ä¸ªé¡¹ç›®çš„æ•°æ®
        for i, post in enumerate(posts[:20]):  # é™åˆ¶å‰20ä¸ªé¡¹ç›®
            try:
                item_data = {
                    'id': f"{target.name.lower()}_{i}_{int(time.time())}",
                    'source': target.name.lower(),
                    'platform': target.name.lower(),
                    'scraped_at': datetime.now().isoformat(),
                    'session_id': self.session_id
                }
                
                # æå–æ ‡é¢˜
                title = await self._extract_field(post, target.selectors.get('title', []))
                if title:
                    item_data['title'] = title.strip()[:300]
                
                # æå–æè¿°
                description = await self._extract_field(post, target.selectors.get('description', []))
                if description:
                    item_data['description'] = description.strip()[:500]
                
                # æå–è¯„åˆ†/æŠ•ç¥¨
                score = await self._extract_field(post, target.selectors.get('score', []))
                if score:
                    item_data['score'] = self._parse_number(score)
                
                votes = await self._extract_field(post, target.selectors.get('votes', []))
                if votes:
                    item_data['votes'] = self._parse_number(votes)
                
                # æå–ä½œè€…
                author = await self._extract_field(post, target.selectors.get('author', []))
                if author:
                    item_data['author'] = author.strip()
                
                # æå–è¯„è®ºæ•°
                comments = await self._extract_field(post, target.selectors.get('comments', []))
                if comments:
                    item_data['comments_count'] = self._parse_number(comments)
                
                # æå–é“¾æ¥
                url = await self._extract_link(post)
                if url:
                    item_data['url'] = url
                
                # åªä¿å­˜æœ‰æ ‡é¢˜çš„é¡¹ç›®
                if item_data.get('title') and len(item_data['title']) > 5:
                    items.append(item_data)
                    print(f"  ğŸ“ {i+1:2d}. {item_data['title'][:60]}...")
                
            except Exception as e:
                print(f"âš ï¸ æå–é¡¹ç›® {i} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"âœ… æ™ºèƒ½æå–å®Œæˆ: {len(items)} æ¡æœ‰æ•ˆæ•°æ®")
        return items
    
    async def _extract_field(self, element, selectors: List[str]) -> Optional[str]:
        """ä»å…ƒç´ ä¸­æå–å­—æ®µ"""
        for selector in selectors:
            try:
                field_element = await element.query_selector(selector)
                if field_element:
                    text = await field_element.text_content()
                    if text and text.strip():
                        return text.strip()
            except:
                continue
        return None
    
    async def _extract_link(self, element) -> Optional[str]:
        """æå–é“¾æ¥"""
        link_selectors = ['a', '[href]', 'a[data-click-id="body"]']
        for selector in link_selectors:
            try:
                link_element = await element.query_selector(selector)
                if link_element:
                    href = await link_element.get_attribute('href')
                    if href:
                        return href if href.startswith('http') else f"https://reddit.com{href}"
            except:
                continue
        return None
    
    def _parse_number(self, text: str) -> int:
        """è§£ææ•°å­—"""
        if not text:
            return 0
        
        # ç§»é™¤éæ•°å­—å­—ç¬¦ï¼Œä¿ç•™k, mç­‰
        text = re.sub(r'[^\d.km]', '', text.lower())
        
        try:
            if 'k' in text:
                return int(float(text.replace('k', '')) * 1000)
            elif 'm' in text:
                return int(float(text.replace('m', '')) * 1000000)
            else:
                return int(float(re.sub(r'[^\d.]', '', text)))
        except:
            return 0
    
    async def scrape_target(self, target_name: str, **kwargs) -> List[Dict[str, Any]]:
        """æŠ“å–æŒ‡å®šç›®æ ‡"""
        if target_name not in self.targets:
            print(f"âŒ æœªçŸ¥ç›®æ ‡: {target_name}")
            return []
        
        target = self.targets[target_name]
        page = await self.context.new_page()
        
        try:
            # æ„å»ºURL
            url = target.url
            if '{}' in url and kwargs.get('subreddit'):
                url = url.format(kwargs['subreddit'])
            
            print(f"\nğŸ¯ æŠ“å–ç›®æ ‡: {target.name}")
            print(f"ğŸŒ è®¿é—®: {url}")
            
            # è®¿é—®é¡µé¢
            await page.goto(url, wait_until='domcontentloaded', timeout=30000)
            
            # æ™ºèƒ½ç­‰å¾…å’ŒåŠ è½½
            await self.smart_wait_and_load(page, target)
            
            # å¤„ç†å¼¹çª—
            if target.anti_detection and target.anti_detection.get('close_popups'):
                await self.handle_popups_and_overlays(page)
            
            # éšæœºé¼ æ ‡ç§»åŠ¨ï¼ˆæ¨¡æ‹ŸçœŸå®ç”¨æˆ·ï¼‰
            if target.anti_detection and target.anti_detection.get('random_mouse_movement'):
                await self._simulate_human_behavior(page)
            
            # æ™ºèƒ½æ•°æ®æå–
            items = await self.smart_extract_data(page, target)
            
            return items
            
        except Exception as e:
            print(f"âŒ æŠ“å– {target.name} å¤±è´¥: {e}")
            await page.screenshot(path=f'{target_name}_error_{self.session_id}.png')
            return []
        
        finally:
            await page.close()
    
    async def _simulate_human_behavior(self, page: Page) -> None:
        """æ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        # éšæœºé¼ æ ‡ç§»åŠ¨
        for _ in range(3):
            x = random.randint(100, 800)
            y = random.randint(100, 600) 
            await page.mouse.move(x, y)
            await asyncio.sleep(random.uniform(0.5, 1.5))
    
    async def run_comprehensive_scraping(self) -> List[Dict[str, Any]]:
        """è¿è¡Œå…¨é¢æŠ“å–"""
        print("ğŸš€ å¯åŠ¨ä¸–ç•Œçº§å…¨é¢æŠ“å–ç³»ç»Ÿ")
        print("=" * 70)
        
        await self.setup_ultimate_browser()
        
        all_results = []
        
        # Reddit å¤šç¤¾åŒºæŠ“å–
        reddit_communities = ['entrepreneur', 'startups', 'SaaS', 'smallbusiness']
        for community in reddit_communities[:2]:  # é™åˆ¶2ä¸ªç¤¾åŒºé¿å…è¿‡é•¿
            print(f"\n{'='*50}")
            items = await self.scrape_target('reddit', subreddit=community)
            all_results.extend(items)
            await asyncio.sleep(random.uniform(3, 5))
        
        # HackerNews
        print(f"\n{'='*50}")
        hn_items = await self.scrape_target('hackernews')
        all_results.extend(hn_items)
        await asyncio.sleep(random.uniform(2, 4))
        
        # Product Hunt
        print(f"\n{'='*50}")
        ph_items = await self.scrape_target('producthunt')
        all_results.extend(ph_items)
        await asyncio.sleep(random.uniform(2, 4))
        
        # IndieHackers
        print(f"\n{'='*50}")
        ih_items = await self.scrape_target('indiehackers')
        all_results.extend(ih_items)
        
        self.results = all_results
        return all_results
    
    def analyze_and_display_results(self) -> None:
        """åˆ†æå’Œå±•ç¤ºç»“æœ"""
        print("\n" + "="*70)
        print("ğŸ‰ ä¸–ç•Œçº§æŠ“å–ç³»ç»Ÿæ‰§è¡Œå®Œæˆ!")
        print("="*70)
        
        if not self.results:
            print("âš ï¸ æœªè·å–åˆ°æ•°æ®")
            return
        
        # ç»Ÿè®¡åˆ†æ
        total_items = len(self.results)
        platforms = {}
        quality_score = 0
        
        for item in self.results:
            platform = item.get('platform', 'unknown')
            platforms[platform] = platforms.get(platform, 0) + 1
            
            # æ•°æ®è´¨é‡è¯„åˆ†
            score = 0
            if item.get('title'): score += 2
            if item.get('description'): score += 1
            if item.get('score') or item.get('votes'): score += 1
            if item.get('author'): score += 1
            if item.get('url'): score += 1
            quality_score += score
        
        avg_quality = quality_score / total_items if total_items > 0 else 0
        
        print(f"ğŸ“Š æŠ“å–ç»Ÿè®¡:")
        print(f"   æ€»æ•°æ®é‡: {total_items} æ¡")
        print(f"   æ•°æ®è´¨é‡è¯„åˆ†: {avg_quality:.1f}/6.0 â­")
        print(f"   ä¼šè¯ID: {self.session_id}")
        
        print(f"\nğŸ“ˆ å¹³å°åˆ†å¸ƒ:")
        for platform, count in sorted(platforms.items()):
            percentage = (count / total_items) * 100
            print(f"   ğŸŒ {platform:15}: {count:3d} æ¡ ({percentage:5.1f}%)")
        
        print(f"\nğŸ“ é«˜è´¨é‡æ•°æ®æ ·æœ¬:")
        # æŒ‰è´¨é‡æ’åºæ˜¾ç¤º
        sorted_items = sorted(self.results, 
                            key=lambda x: len(str(x.get('title', ''))), 
                            reverse=True)
        
        for i, item in enumerate(sorted_items[:10]):
            platform = item.get('platform', '').upper()
            title = item.get('title', 'No title')[:60]
            score = item.get('score', item.get('votes', 0))
            quality_indicators = []
            
            if item.get('description'): quality_indicators.append('ğŸ“')
            if item.get('author'): quality_indicators.append('ğŸ‘¤')
            if item.get('url'): quality_indicators.append('ğŸ”—')
            if score: quality_indicators.append(f'â­{score}')
            
            indicators = ' '.join(quality_indicators)
            print(f"  {i+1:2d}. [{platform:12}] {title}... {indicators}")
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"ultimate_scraping_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                'session_id': self.session_id,
                'timestamp': timestamp,
                'total_items': total_items,
                'quality_score': avg_quality,
                'platforms': platforms,
                'data': self.results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        print(f"â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    async def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        print("\nğŸ§¹ æ¸…ç†ä¸–ç•Œçº§æµè§ˆå™¨èµ„æº...")
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        print("âœ… æ¸…ç†å®Œæˆ")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ ä¸–ç•Œçº§æµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ“å–ç³»ç»Ÿ")
    print("ğŸ•·ï¸ ç”±é¡¶çº§çˆ¬è™«ä¸“å®¶è®¾è®¡")
    print(f"â° å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    scraper = WorldClassBrowserScraper()
    
    try:
        await scraper.run_comprehensive_scraping()
        scraper.analyze_and_display_results()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await scraper.cleanup()


if __name__ == "__main__":
    asyncio.run(main())